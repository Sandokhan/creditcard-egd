{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4df6548-1dda-4e79-8816-8085effd90f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ENVIRONMENT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7925d96-e7ad-462c-9c28-143d5a4e456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('You can run the script automatically or iteratively. The automatic mode will run in the spark cluster, execute data profiling, executing ML training and testing, without data visualization') \n",
    "autoExec = input('Automatic Execution (Y-yes , N-no)?: ').lower()=='y'\n",
    "# autoExec = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a88c0c-db0f-40ab-88ca-d2cc306975d2",
   "metadata": {
    "id": "73a88c0c-db0f-40ab-88ca-d2cc306975d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application_1652382646560_0297\n",
      "http://sparkhistoryserver.hdp:18080/history/application_1652382646560_0297\n"
     ]
    }
   ],
   "source": [
    "#TODO: remove unnecessary imports\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql import SQLContext\n",
    "#from pyspark.sql.functions import col\n",
    "#from pyspark.sql.types import StringType,BooleanType,DateType, NumericType, DoubleType, StructType, IntegerType\n",
    "\n",
    "if autoExec:\n",
    "    localExec = False\n",
    "else: \n",
    "    localExec = input('Local Execution (Y-yes , N-no)?: ').lower()=='y'\n",
    "\n",
    "if localExec:\n",
    "    spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[8]\")\\\n",
    "    .config(\"spark.ui.port\", \"4041\")\\\n",
    "    .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.instances\", \"4\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "        .config(\"spark.eventLog.dir\", \"hdfs:///spark-history\") \\\n",
    "        .config(\"spark.default.parallelism\", \"4\") \\\n",
    "        .master(\"yarn\")\\\n",
    "        .appName(\"egd group1 final_4-4-4-4\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "app_id = spark._sc.applicationId\n",
    "print(app_id)\n",
    "print(\"http://sparkhistoryserver.hdp:18080/history/\" + app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9657f47-c4a6-4b07-b7cb-533b987963be",
   "metadata": {
    "id": "f9657f47-c4a6-4b07-b7cb-533b987963be"
   },
   "outputs": [],
   "source": [
    "#TODO: remove in final version\n",
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d2f92-d04d-4b7d-964d-c11b67b18e26",
   "metadata": {
    "id": "bd5d2f92-d04d-4b7d-964d-c11b67b18e26",
    "tags": []
   },
   "source": [
    "# INGESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff7efa-af9a-4f70-a154-4ef27bd32d31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f98130a-7a69-429c-8c53-fdfc1caa159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if localExec:\n",
    "    # data input local FS\n",
    "    path = \"./data/2018_10k.txt\"\n",
    "    rdd0 = spark.sparkContext.textFile(path)\n",
    "else:\n",
    "    # data input HDFS\n",
    "    path = \"hdfs:///user/group1/2018_nh.csv\"\n",
    "    rdd0 = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c074de-6d7e-421a-997b-002597745e5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e2c947-2c00-4516-b2e9-1108a50688a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 7213446\n",
      "Total number of columns: 28\n",
      "Data sample:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2018-01-01,UA,2429,EWR,DEN,1517,1512.0,-5.0,15.0,1527.0,1712.0,10.0,1745,1722.0,-23.0,0.0,,0.0,268.0,250.0,225.0,1605.0,,,,,,',\n",
       " '2018-01-01,UA,2427,LAS,SFO,1115,1107.0,-8.0,11.0,1118.0,1223.0,7.0,1254,1230.0,-24.0,0.0,,0.0,99.0,83.0,65.0,414.0,,,,,,',\n",
       " '2018-01-01,UA,2426,SNA,DEN,1335,1330.0,-5.0,15.0,1345.0,1631.0,5.0,1649,1636.0,-13.0,0.0,,0.0,134.0,126.0,106.0,846.0,,,,,,',\n",
       " '2018-01-01,UA,2425,RSW,ORD,1546,1552.0,6.0,19.0,1611.0,1748.0,6.0,1756,1754.0,-2.0,0.0,,0.0,190.0,182.0,157.0,1120.0,,,,,,',\n",
       " '2018-01-01,UA,2424,ORD,ALB,630,650.0,20.0,13.0,703.0,926.0,10.0,922,936.0,14.0,0.0,,0.0,112.0,106.0,83.0,723.0,,,,,,',\n",
       " '2018-01-01,UA,2422,ORD,OMA,2241,2244.0,3.0,15.0,2259.0,1.0,2.0,14,3.0,-11.0,0.0,,0.0,93.0,79.0,62.0,416.0,,,,,,',\n",
       " '2018-01-01,UA,2421,IAH,LAS,750,747.0,-3.0,14.0,801.0,854.0,6.0,916,900.0,-16.0,0.0,,0.0,206.0,193.0,173.0,1222.0,,,,,,',\n",
       " '2018-01-01,UA,2420,DEN,CID,1324,1318.0,-6.0,11.0,1329.0,1554.0,6.0,1619,1600.0,-19.0,0.0,,0.0,115.0,102.0,85.0,692.0,,,,,,',\n",
       " '2018-01-01,UA,2419,SMF,EWR,2224,2237.0,13.0,10.0,2247.0,627.0,9.0,638,636.0,-2.0,0.0,,0.0,314.0,299.0,280.0,2500.0,,,,,,',\n",
       " '2018-01-01,UA,2418,RIC,DEN,1601,1559.0,-2.0,12.0,1611.0,1748.0,8.0,1813,1756.0,-17.0,0.0,,0.0,252.0,237.0,217.0,1482.0,,,,,,']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial profiling\n",
    "dpTotalNrRecords = rdd0.count()\n",
    "print(f'Total number of records: {dpTotalNrRecords}')\n",
    "\n",
    "dpTotalNrColumns = len(rdd0.take(1)[0].split(\",\"))\n",
    "print(f'Total number of columns: {dpTotalNrColumns}')\n",
    "\n",
    "print('Data sample:')\n",
    "rdd0.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd38da68-f22b-49fe-9505-c2a45fff8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data type conversion aux functions\n",
    "from datetime import datetime\n",
    "\n",
    "def txt2float(t):\n",
    "    s = t.strip()\n",
    "    fvalue = 0\n",
    "    if s == '':\n",
    "        return None\n",
    "    try:\n",
    "        fvalue = float(s)\n",
    "    except:\n",
    "        fvalue = None\n",
    "    \n",
    "    return fvalue\n",
    "\n",
    "def txt2str(t):\n",
    "    s = t.strip()\n",
    "    if s == '':\n",
    "        return None\n",
    "    \n",
    "    return str(t)\n",
    "\n",
    "def txt2date(t):\n",
    "    s = t.strip()\n",
    "    dvalue = None\n",
    "    \n",
    "    if s == '':\n",
    "        return None\n",
    "    try:\n",
    "        dvalue = datetime.strptime(s, '%Y-%m-%d').date()\n",
    "    except:\n",
    "        dvalue = None\n",
    "\n",
    "    return dvalue\n",
    "\n",
    "def filterByIndexT(li, idx):\n",
    "    return [(i,li[i]) for i in idx]\n",
    "\n",
    "def filterByIndexV(li, idx):\n",
    "    return [li[i] for i in idx]\n",
    "\n",
    "# data type conversion\n",
    "inSchema = {\n",
    "    0:txt2date,   #FL_DATE\n",
    "    1:txt2str,    #OP_CARRIER\n",
    "    2:txt2str,    #OP_CARRIER_FL_NUM\n",
    "    3:txt2str,    #ORIGIN\n",
    "    4:txt2str,    #DEST\n",
    "    5:txt2float,  #CRS_DEP_TIME\n",
    "    6:txt2float,  #DEP_TIME\n",
    "    7:txt2float,  #DEP_DELAY\n",
    "    8:txt2float,  #TAXI_OUT\n",
    "    9:txt2float,  #WHEELS_OFF\n",
    "    10:txt2float, #WHEELS_ON\n",
    "    11:txt2float, #TAXI_IN\n",
    "    12:txt2float, #CRS_ARR_TIME\n",
    "    13:txt2float, #ARR_TIME\n",
    "    14:txt2float, #ARR_DELAY\n",
    "    15:txt2float, #CANCELLED\n",
    "    16:txt2str,   #CANCELLED REASON\n",
    "    17:txt2float, #DIVERTED\n",
    "    18:txt2float, #CRS_ELAPSED_TIME\n",
    "    19:txt2float, #ACTUAL_ELAPSED_TIME\n",
    "    20:txt2float, #AIR_TIME\n",
    "    21:txt2float  #DISTANCE\n",
    "    # 22: REL_INIT_DELAY\n",
    "    # 23: T_LATE\n",
    "    # 24: T_REL_ELAP_TIME\n",
    "    # 25: FL_DATE_DAY_IN_WEEK\n",
    "    # 26: FL_DATE_DAY_IN_MONTH\n",
    "    # 27: FL_DATE_MONTH\n",
    "    # 28: DEP_HOUR_IN_DAY\n",
    "    # 29: ARR_HOUR_IN_DAY\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a692187b-f37f-42da-8090-a2fa1dc4b732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[5] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse file\n",
    "rdd2 = rdd0.map(lambda x: x.split(','))\n",
    "\n",
    "# data type conversion based on data schema\n",
    "rdd3 = rdd2.map(lambda x: [inSchema[idx](val) for (idx,val) in filterByIndexT(x, list(inSchema.keys()))])\n",
    "rdd3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "149947b7-7ca3-4538-b960-3b9cfad2294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data profiling aux functions\n",
    "\n",
    "# nr of records, excl missing values\n",
    "def dpCount(rdd, idx):\n",
    "    return rdd.filter(lambda x: x[idx] is not None).count()\n",
    "\n",
    "# mean value\n",
    "def dpMean(rdd, idx):\n",
    "    val = rdd\\\n",
    "        .filter(lambda x: x[idx] is not None)\\\n",
    "        .map(lambda x: (1, x[idx]))\\\n",
    "        .reduce(lambda x,y: (x[0]+y[0],x[1]+y[1]))\n",
    "    \n",
    "    return round(val[1]/val[0],3)\n",
    "\n",
    "# max value\n",
    "def dpMax(rdd, idx):\n",
    "    val = rdd\\\n",
    "        .filter(lambda x: x[idx] is not None)\\\n",
    "        .map(lambda x: x[idx])\\\n",
    "        .reduce(lambda x,y: x if x > y else y)\n",
    "    \n",
    "    return round(val, 3)\n",
    "\n",
    "# min value\n",
    "def dpMin(rdd, idx):\n",
    "    val = rdd\\\n",
    "        .filter(lambda x: x[idx] is not None)\\\n",
    "        .map(lambda x: x[idx])\\\n",
    "        .reduce(lambda x,y: x if x < y else y)\n",
    "    \n",
    "    return round(val, 3)\n",
    "\n",
    "# proportions (categ attributes)\n",
    "def dpProp(rdd, idx):\n",
    "    val = rdd\\\n",
    "        .filter(lambda x: x[idx] is not None)\\\n",
    "        .map(lambda x: (x[idx],1))\\\n",
    "        .reduceByKey(lambda x, y: x+y)\\\n",
    "        .collect()\n",
    "    \n",
    "    return {key:count for (key, count) in val}\n",
    "\n",
    "# variance\n",
    "def dpVar(rdd, idx):\n",
    "    \n",
    "    avg = dpMean(rdd, idx)\n",
    "    \n",
    "    val = rdd\\\n",
    "        .filter(lambda x: x[idx] is not None)\\\n",
    "        .map(lambda x: (1, pow(x[idx]-avg,2)))\\\n",
    "        .reduce(lambda x,y: (x[0]+y[0],x[1]+y[1]))\n",
    "    \n",
    "    return round(val[1]/val[0],3)\n",
    "\n",
    "# nr of distinct values (categ attributes)\n",
    "def dpDist(rdd, idx):\n",
    "    return rdd.map(lambda x: x[idx]).distinct().count()\n",
    "\n",
    "# covariance\n",
    "def dpCovar(rdd, idx1, idx2):\n",
    "    \n",
    "    avg1 = dpMean(rdd, idx1)\n",
    "    avg2 = dpMean(rdd, idx2)\n",
    "    \n",
    "    val = rdd\\\n",
    "        .filter(lambda x: x[idx1] is not None and x[idx2] is not None)\\\n",
    "        .map(lambda x: (1, (x[idx1]-avg1)*(x[idx2]-avg2)))\\\n",
    "        .reduce(lambda x,y: (x[0]+y[0],x[1]+y[1]))\n",
    "    \n",
    "    return round(val[1]/val[0],3)\n",
    "\n",
    "# correlation\n",
    "def dpCorr(rdd, idx1, idx2):\n",
    "    return round(dpCovar(rdd, idx1, idx2) / (math.sqrt(dpVar(rdd, idx1) * dpVar(rdd, idx2))),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2291ffd1-602b-4fdc-b342-0a56afd05004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data statistics (smaller sample / original dataset):\n",
      "FL_DATE:\n",
      "\n",
      "\t| count: 7213446\n",
      "\n",
      "\t| distinct: 365\n",
      "\n",
      "OP_CARRIER:\n",
      "\n",
      "\t| count: 7213446\n",
      "\n",
      "\t| distinct: 18\n",
      "\n",
      "OP_CARRIER_FL_NUM:\n",
      "\n",
      "\t| count: 7213446\n",
      "\n",
      "\t| distinct: 7113\n",
      "\n",
      "ORIGIN:\n",
      "\n",
      "\t| count: 7213446\n",
      "\n",
      "\t| distinct: 358\n",
      "\n",
      "DEST:\n",
      "\n",
      "\t| count: 7213446\n",
      "\n",
      "\t| distinct: 358\n",
      "\n",
      "CRS_DEP_TIME:\n",
      "\n",
      "\t| count: 7213446\n",
      "\n",
      "DEP_TIME:\n",
      "\n",
      "\t| count: 7101129\n",
      "\n",
      "\t| min: 1.0\n",
      "\n",
      "\t| max: 2400.0\n",
      "\n",
      "DEP_DELAY:\n",
      "\n",
      "\t| count: 7096212\n",
      "\n",
      "\t| mean: 9.97\n",
      "\n",
      "\t| min: -122.0\n",
      "\n",
      "\t| max: 2710.0\n",
      "\n",
      "\t| stdev: 44.83\n",
      "\n",
      "TAXI_OUT:\n",
      "\n",
      "\t| count: 7097616\n",
      "\n",
      "\t| mean: 17.411\n",
      "\n",
      "\t| min: 1.0\n",
      "\n",
      "\t| max: 196.0\n",
      "\n",
      "\t| stdev: 9.92\n",
      "\n",
      "WHEELS_OFF:\n",
      "\n",
      "\t| count: 7097617\n",
      "\n",
      "WHEELS_ON:\n",
      "\n",
      "\t| count: 7094200\n",
      "\n",
      "TAXI_IN:\n",
      "\n",
      "\t| count: 7094200\n",
      "\n",
      "\t| mean: 7.601\n",
      "\n",
      "\t| min: 1.0\n",
      "\n",
      "\t| max: 259.0\n",
      "\n",
      "\t| stdev: 6.065\n",
      "\n",
      "CRS_ARR_TIME:\n",
      "\n",
      "\t| count: 7213446\n",
      "\n",
      "\t| min: 1.0\n",
      "\n",
      "\t| max: 2400.0\n",
      "\n",
      "ARR_TIME:\n",
      "\n",
      "\t| count: 7094201\n",
      "\n",
      "\t| min: 1.0\n",
      "\n",
      "\t| max: 2400.0\n",
      "\n",
      "ARR_DELAY:\n",
      "\n",
      "\t| count: 7076406\n",
      "\n",
      "\t| mean: 5.049\n",
      "\n",
      "\t| min: -120.0\n",
      "\n",
      "\t| max: 2692.0\n",
      "\n",
      "\t| stdev: 46.927\n",
      "\n",
      "CANCELLED:\n",
      "\n",
      "\t| count: 7213446\n",
      "\n",
      "\t| prop: {0.0: 7096862, 1.0: 116584}\n",
      "\n",
      "CANCELLED REASON:\n",
      "\n",
      "\t| count: 116584\n",
      "\n",
      "\t| prop: {'B': 61984, 'A': 29484, 'C': 25072, 'D': 44}\n",
      "\n",
      "DIVERTED:\n",
      "\n",
      "\t| count: 7213446\n",
      "\n",
      "\t| prop: {0.0: 7195587, 1.0: 17859}\n",
      "\n",
      "CRS_ELAP_TIME:\n",
      "\n",
      "\t| count: 7213436\n",
      "\n",
      "\t| mean: 141.136\n",
      "\n",
      "\t| min: -99.0\n",
      "\n",
      "\t| max: 704.0\n",
      "\n",
      "\t| stdev: 73.344\n",
      "\n",
      "ACT_ELAP_TIME:\n",
      "\n",
      "\t| count: 7079004\n",
      "\n",
      "\t| mean: 136.5\n",
      "\n",
      "\t| min: 14.0\n",
      "\n",
      "\t| max: 757.0\n",
      "\n",
      "\t| stdev: 73.138\n",
      "\n",
      "AIR_TIME:\n",
      "\n",
      "\t| count: 7079004\n",
      "\n",
      "\t| min: 7.0\n",
      "\n",
      "\t| max: 696.0\n",
      "\n",
      "\t| stdev: 71.113\n",
      "\n",
      "DISTANCE:\n",
      "\n",
      "\t| count: 7213446\n",
      "\n",
      "\t| mean: 799.989\n",
      "\n",
      "\t| min: 31.0\n",
      "\n",
      "\t| max: 4983.0\n",
      "\n",
      "\t| stdev: 598.178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# data profilling\n",
    "# NOTE: run only when profilling data\n",
    "\n",
    "profillingOps = {\n",
    "    0: ('FL_DATE',            [('count',dpCount), ('distinct',dpDist)]),\n",
    "    1: ('OP_CARRIER',         [('count',dpCount), ('distinct',dpDist)]),\n",
    "    2: ('OP_CARRIER_FL_NUM',  [('count',dpCount), ('distinct',dpDist)]),\n",
    "    3: ('ORIGIN',             [('count',dpCount), ('distinct',dpDist)]), \n",
    "    4: ('DEST',               [('count',dpCount), ('distinct',dpDist)]), \n",
    "    5: ('CRS_DEP_TIME',       [('count',dpCount)]),\n",
    "    6: ('DEP_TIME',           [('count',dpCount),('min',dpMin),('max',dpMax)]),\n",
    "    7: ('DEP_DELAY',          [('count',dpCount),('mean',dpMean),('min',dpMin),('max',dpMax),('stdev',lambda x,y: round(math.sqrt(dpVar(x,y)),3))]),\n",
    "    8: ('TAXI_OUT',           [('count',dpCount),('mean',dpMean),('min',dpMin),('max',dpMax),('stdev',lambda x,y: round(math.sqrt(dpVar(x,y)),3))]),\n",
    "    9: ('WHEELS_OFF',         [('count',dpCount)]),\n",
    "    10:('WHEELS_ON',          [('count',dpCount)]),\n",
    "    11:('TAXI_IN',            [('count',dpCount),('mean',dpMean),('min',dpMin),('max',dpMax),('stdev',lambda x,y: round(math.sqrt(dpVar(x,y)),3))]),\n",
    "    12:('CRS_ARR_TIME',       [('count',dpCount),('min',dpMin),('max',dpMax)]),\n",
    "    13:('ARR_TIME',           [('count',dpCount),('min',dpMin),('max',dpMax)]),\n",
    "    14:('ARR_DELAY',          [('count',dpCount),('mean',dpMean),('min',dpMin),('max',dpMax),('stdev',lambda x,y: round(math.sqrt(dpVar(x,y)),3))]),\n",
    "    15:('CANCELLED',          [('count',dpCount),('prop',dpProp)]),\n",
    "    16:('CANCELLED REASON',   [('count',dpCount),('prop',dpProp)]),\n",
    "    17:('DIVERTED',           [('count',dpCount),('prop',dpProp)]),\n",
    "    18:('CRS_ELAP_TIME',      [('count',dpCount),('mean',dpMean),('min',dpMin),('max',dpMax),('stdev',lambda x,y: round(math.sqrt(dpVar(x,y)),3))]),\n",
    "    19:('ACT_ELAP_TIME',      [('count',dpCount),('mean',dpMean),('min',dpMin),('max',dpMax),('stdev',lambda x,y: round(math.sqrt(dpVar(x,y)),3))]),\n",
    "    20:('AIR_TIME',           [('count',dpCount),('min',dpMin),('max',dpMax),('stdev',lambda x,y: round(math.sqrt(dpVar(x,y)),3))]),\n",
    "    21:('DISTANCE',           [('count',dpCount),('mean',dpMean),('min',dpMin),('max',dpMax),('stdev',lambda x,y: round(math.sqrt(dpVar(x,y)),3))])\n",
    "}\n",
    "\n",
    "def dpPrettyPrint(dpStatistics):\n",
    "    for attrib, statsList in dpStatistics.items():\n",
    "        print(f'{attrib}:\\n')\n",
    "        for metric, value in statsList.items():\n",
    "            print(f'\\t| {metric}: {value}\\n')\n",
    "\n",
    "if autoExec:\n",
    "    dpExec = True\n",
    "else: \n",
    "    dpExec = input('Data Profiling (Y-yes , N-no)?: ').lower()=='y'\n",
    "    \n",
    "if dpExec:\n",
    "    stats = {opers[0]:{ operName:operFunc(rdd3, idx) for operName, operFunc in opers[1]} for (idx,opers) in profillingOps.items()}\n",
    "    print('Data statistics (smaller sample / original dataset):')\n",
    "    dpPrettyPrint(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a2d0be-a9b7-4373-8029-097599a32280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Statistics - Correlations:\n",
      "\tDEP_DELAY x ARR_TIME: 0.03\n",
      "\tDEP_DELAY x CRS_ELAP_TIME: 0.013\n",
      "\tDEP_DELAY x ACT_ELAP_TIME: 0.017\n",
      "\tDEP_DELAY x AIR_TIME: 0.009\n",
      "\tDEP_DELAY x DISTANCE: 0.009\n",
      "\tARR_TIME x CRS_ELAP_TIME: 0.013\n",
      "\tARR_TIME x ACT_ELAP_TIME: 0.015\n",
      "\tARR_TIME x AIR_TIME: 0.011\n",
      "\tARR_TIME x DISTANCE: 0.006\n",
      "\tCRS_ELAP_TIME x ACT_ELAP_TIME: 0.985\n",
      "\tCRS_ELAP_TIME x AIR_TIME: 0.991\n",
      "\tCRS_ELAP_TIME x DISTANCE: 0.983\n",
      "\tACT_ELAP_TIME x AIR_TIME: 0.987\n",
      "\tACT_ELAP_TIME x DISTANCE: 0.971\n",
      "\tAIR_TIME x DISTANCE: 0.987\n"
     ]
    }
   ],
   "source": [
    "# correlation calculation\n",
    "\n",
    "if dpExec:\n",
    "    dpCovrAttribs = filterByIndexV([(idx,val[0]) for idx, val in profillingOps.items()],[7, 13, 18, 19, 20, 21])\n",
    "\n",
    "    dpCorrRes = {}\n",
    "    for i,iName in dpCovrAttribs:\n",
    "        for j,jName in dpCovrAttribs:\n",
    "            key, revKey = f'{iName} x {jName}', f'{jName} x {iName}'\n",
    "            if revKey not in dpCorrRes.keys():\n",
    "                if i!=j:\n",
    "                    dpCorrRes[key] = dpCorr(rdd3, i,j)\n",
    "\n",
    "    print('Data Statistics - Correlations:')\n",
    "    for key, value in dpCorrRes.items():\n",
    "        print(f'\\t{key}: {value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ebdae-4f1c-4da4-9daf-ebaedf29a815",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2ed9495-c3b6-4db0-92b5-b82cab24d382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of removed records with missing values: 141629\n",
      "Total number of removed records based on inclusion criteria: 4634767\n",
      "Total number of removed records with outliers: 417698\n",
      "Total number of resulting records: 2019352 (27.99%)\n"
     ]
    }
   ],
   "source": [
    "# instance filtering: instances with relevant missing attributes\n",
    "idxRelProc = [0,1,3,4,6,7,8,11,12,14,15,17,18,19,21] #attribs relevant for missing value proc\n",
    "rdd4 = rdd3.filter(lambda x: None not in filterByIndexV(x, idxRelProc))\n",
    "\n",
    "dpNrRecordsActual = dpTotalNrRecords\n",
    "dpNrOfRecordsAfterRemoval = rdd4.count()\n",
    "print(f'Total number of removed records with missing values: {dpNrRecordsActual-dpNrOfRecordsAfterRemoval}')\n",
    "dpNrRecordsActual = dpNrOfRecordsAfterRemoval\n",
    "\n",
    "# instance filtering: cancelled, diverted and on-time departure fligths\n",
    "rdd5 = rdd4\\\n",
    "    .filter(lambda x: x[15]!=1)\\\n",
    "    .filter(lambda x: x[17]!=1)\\\n",
    "    .filter(lambda x: x[7]>0)\n",
    "\n",
    "dpNrOfRecordsAfterRemoval = rdd5.count()\n",
    "print(f'Total number of removed records based on inclusion criteria: {dpNrRecordsActual-dpNrOfRecordsAfterRemoval}')\n",
    "dpNrRecordsActual = dpNrOfRecordsAfterRemoval\n",
    "\n",
    "# exclude outliers in ARR_DELAY\n",
    "rdd6 = rdd5.sortBy(lambda x: x[14]).cache()\n",
    "cnt = rdd6.count()\n",
    "idx25 = int(cnt*0.25) - 1\n",
    "idx75 = int(cnt*0.75) - 1\n",
    "iqr = rdd6.zipWithIndex().filter(lambda x: x[1] in [idx25, idx75]).reduce(lambda x, y: abs(x[0][14] - y[0][14]))\n",
    "rdd7 = rdd6.filter(lambda x: x[14] < iqr * 1.5 and x[14] > iqr * -1.5)\n",
    "\n",
    "dpNrOfRecordsAfterRemoval = rdd7.count()\n",
    "print(f'Total number of removed records with outliers: {dpNrRecordsActual-dpNrOfRecordsAfterRemoval}')\n",
    "dpNrRecordsActual = dpNrOfRecordsAfterRemoval\n",
    "\n",
    "print(f'Total number of resulting records: {dpNrRecordsActual} ({round((dpNrRecordsActual/dpTotalNrRecords)*100,2)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40434695-3775-4fb2-b7a9-db3afe07f62d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a6f221-bc1a-49bb-8f5d-d3a1d2c725e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculated attributes: \n",
    "# 1. REL_INIT_DELAY = (DEP_DELAY + TAXI_OUT) / CRS_ELAPSED_TIME\n",
    "# 2. T_LATE = 1 if ACTUAL_ELAPSED_TIME - CRS_ELAPSED_TIME > 15, 0 otherwise\n",
    "# 3. T_REL_ELAP_TIME = ACTUAL_ELAPSED_TIME / frdd.CRS_ELAPSED_TIME\n",
    "rdd8 = rdd7\\\n",
    "    .map(lambda x: x + [(x[7]+x[8])/x[18]])\\\n",
    "    .map(lambda x: x + [1 if x[19]-x[18] > 15 else 0])\\\n",
    "    .map(lambda x: x + [x[19]/x[18]])\n",
    "\n",
    "# derived attributes:\n",
    "# 1. FL_DATE_DAY_IN_WEEK\n",
    "# 2. FL_DATE_DAY_IN_MONTH\n",
    "# 3. FL_DATE_MONTH\n",
    "rdd9 = rdd8\\\n",
    "    .map(lambda x: x + [x[0].weekday()])\\\n",
    "    .map(lambda x: x + [x[0].day])\\\n",
    "    .map(lambda x: x + [x[0].month])\n",
    "\n",
    "# calculated attributes:\n",
    "# 1. DEP_HOUR_IN_DAY from DEP_TIME\n",
    "# 2. ARR_HOUR_IN_DAY from CRS_ARR_TIME\n",
    "rdd10 = rdd9\\\n",
    "    .map(lambda x: x + [x[6]//100])\\\n",
    "    .map(lambda x: x + [x[12]//100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ef97e-f40c-425b-a68a-7bb0f652a72c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07092e14-87b4-4950-9343-b82c4c88104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the attributes not needed for analysis\n",
    "idxRelAna = [1,3,4,14,18,21,22,23,24,25,26,27,28,29] # attribs relevant for analysis\n",
    "rdd11 = rdd10.map(lambda x: filterByIndexV(x, idxRelAna))\n",
    "\n",
    "# convert RDD to DF\n",
    "outSchemaL = [\"OP_CARRIER\",\n",
    "              \"ORIGIN\",\n",
    "              \"DEST\",\n",
    "              \"ARR_DELAY\",\n",
    "              \"CRS_ELAPSED_TIME\",\n",
    "              \"DISTANCE\",\n",
    "              \"REL_INIT_DELAY\",\n",
    "              \"T_LATE\",\n",
    "              \"T_REL_ELAP_TIME\",\n",
    "              \"FL_DATE_DAY_IN_WEEK\",\n",
    "              \"FL_DATE_DAY_IN_MONTH\",\n",
    "              \"FL_DATE_MONTH\",\n",
    "              \"DEP_HOUR_IN_DAY\",\n",
    "              \"ARR_HOUR_IN_DAY\"]\n",
    "\n",
    "df = rdd11.toDF(outSchemaL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629a456-4da7-4ae8-9640-7166622d189a",
   "metadata": {
    "id": "c629a456-4da7-4ae8-9640-7166622d189a",
    "tags": []
   },
   "source": [
    "# QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "746ccbf1-7fbf-4adb-9c24-105ab08d1fce",
   "metadata": {
    "id": "746ccbf1-7fbf-4adb-9c24-105ab08d1fce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+\n",
      "|ORIGIN|DEST|count|\n",
      "+------+----+-----+\n",
      "|   LAX| SFO| 4154|\n",
      "|   SFO| LAX| 4048|\n",
      "|   ORD| LGA| 3969|\n",
      "|   LAS| LAX| 3657|\n",
      "|   LAX| JFK| 3605|\n",
      "|   LAX| LAS| 3552|\n",
      "|   LGA| ORD| 3496|\n",
      "|   ATL| LGA| 3448|\n",
      "|   ATL| MCO| 3410|\n",
      "|   ATL| FLL| 3363|\n",
      "+------+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 10 flight routes\n",
    "df.groupBy('ORIGIN','DEST').count()\\\n",
    "    .orderBy('count',ascending=False)\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38191416-b88c-4ffb-911e-6f6afb5dff64",
   "metadata": {
    "id": "38191416-b88c-4ffb-911e-6f6afb5dff64"
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17c63285-f6e1-4718-beb0-07c374862982",
   "metadata": {
    "id": "17c63285-f6e1-4718-beb0-07c374862982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+\n",
      "|    QTD|Arrival_Flight_Delays|\n",
      "+-------+---------------------+\n",
      "|1340757|     Tolerable Delays|\n",
      "|  50152|            No Delays|\n",
      "| 605771|                Early|\n",
      "|  22672|         Short Delays|\n",
      "+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification of delays\n",
    "# TODO: Review intervals, review the ordering, order by qty\n",
    "\n",
    "spark.sql(\"\"\"SELECT COUNT(1) AS QTD, Arrival_Flight_Delays FROM (\n",
    "SELECT ARR_DELAY, ORIGIN, DEST, \n",
    "              CASE\n",
    "                  WHEN ARR_DELAY > 360 THEN 'Very Long Delays'\n",
    "                  WHEN ARR_DELAY >= 120 AND ARR_DELAY <= 360 THEN 'Long Delays'\n",
    "                  WHEN ARR_DELAY >= 60 AND ARR_DELAY < 120 THEN 'Short Delays'\n",
    "                  WHEN ARR_DELAY > 0 and ARR_DELAY < 60 THEN 'Tolerable Delays'\n",
    "                  WHEN ARR_DELAY = 0 THEN 'No Delays'\n",
    "                  ELSE 'Early'\n",
    "               END AS Arrival_Flight_Delays\n",
    "               FROM df\n",
    "               ORDER BY ARR_DELAY DESC) AS A \n",
    "            GROUP BY Arrival_Flight_Delays\n",
    "               \"\"\").show(10) ## RDD = 9 s + 47 ms + 75 ms + 0.3 s + 0.2 s = 9.62s\n",
    "                             ## Cache = 0.3 s + 51 ms + 0.1 s + 0.2 s + 0.2 s = 0.85s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71f33235-3069-43b0-9a40-5d5bba36051c",
   "metadata": {
    "id": "71f33235-3069-43b0-9a40-5d5bba36051c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|T_LATE|FLIGHTS_COUNT|\n",
      "+------+-------------+\n",
      "|     0|      1921186|\n",
      "|     1|        98166|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#proportion of flights with delays\n",
    "spark.sql(\"\"\"select T_LATE, count(1) FLIGHTS_COUNT \n",
    "            from DF group by T_LATE\"\"\").show(10)\n",
    "## RDD = 12 s + 37 ms + 0.1 s + 0.7 s + 0.6 s = 13,43s\n",
    "## Cache = 0.2 s + 27 ms + 49 ms + 0.2 s + 0.1 s = 0.57s                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63524146-a225-42f0-9809-e9ab6f7972fa",
   "metadata": {
    "id": "63524146-a225-42f0-9809-e9ab6f7972fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|FL_DATE_DAY_IN_WEEK|     AVG_REL_DELAY|\n",
      "+-------------------+------------------+\n",
      "|                  0|1.2058198579991954|\n",
      "|                  1|1.2052453830417045|\n",
      "|                  2|1.2029307448047917|\n",
      "|                  3|1.1999864207081972|\n",
      "|                  4| 1.201655428258325|\n",
      "|                  5|1.1948468573064042|\n",
      "|                  6|1.2025400101303374|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##considering the delayed flights average relative delay per D/W\n",
    "spark.sql(\"\"\"select \n",
    "                FL_DATE_DAY_IN_WEEK, \n",
    "                avg(T_REL_ELAP_TIME) AVG_REL_DELAY\n",
    "            from DF\n",
    "            where T_LATE = 1\n",
    "            group by FL_DATE_DAY_IN_WEEK \n",
    "            order by FL_DATE_DAY_IN_WEEK asc\"\"\").show(10) ## RDD = 10 s, cache = 0.8 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38d88018-36a4-4687-8d63-d077e5267b91",
   "metadata": {
    "id": "38d88018-36a4-4687-8d63-d077e5267b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|FL_DATE_DAY_IN_MONTH|     AVG_REL_DELAY|\n",
      "+--------------------+------------------+\n",
      "|                   1|1.1965535847918214|\n",
      "|                   2|1.2040730760590717|\n",
      "|                   3| 1.206615735203679|\n",
      "|                   4|1.2005635261365217|\n",
      "|                   5| 1.201093686449931|\n",
      "|                   6|1.2052568391294207|\n",
      "|                   7|1.2068821445295985|\n",
      "|                   8|1.1989050285957803|\n",
      "|                   9| 1.208585095817865|\n",
      "|                  10|  1.19708517081429|\n",
      "+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 10 relative delayed flights per day in the week\n",
    "spark.sql(\"\"\"select \n",
    "                FL_DATE_DAY_IN_MONTH, \n",
    "                avg(T_REL_ELAP_TIME) AVG_REL_DELAY\n",
    "            from DF\n",
    "            where T_LATE = 1\n",
    "            group by FL_DATE_DAY_IN_MONTH \n",
    "            order by FL_DATE_DAY_IN_MONTH asc\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa2c8539-1f65-4c7e-b15a-c7febbb23da0",
   "metadata": {
    "id": "aa2c8539-1f65-4c7e-b15a-c7febbb23da0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|FL_DATE_MONTH|     AVG_REL_DELAY|\n",
      "+-------------+------------------+\n",
      "|            1|1.2143748592950934|\n",
      "|            2|1.2052373094845752|\n",
      "|            3|1.1988932333568176|\n",
      "|            4|1.2027828186801572|\n",
      "|            5|1.2029547211539404|\n",
      "|            6|1.2010854200850174|\n",
      "|            7|1.1978532703795364|\n",
      "|            8|1.2018947448085386|\n",
      "|            9|1.2071519172314917|\n",
      "|           10|1.1969102796743984|\n",
      "|           11|1.2057191087917536|\n",
      "|           12|1.1947725148710568|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##average relative delay per Month\n",
    "spark.sql(\"\"\"select \n",
    "                FL_DATE_MONTH, \n",
    "                avg(T_REL_ELAP_TIME\n",
    "                ) AVG_REL_DELAY\n",
    "            from DF\n",
    "            where T_LATE = 1\n",
    "            group by FL_DATE_MONTH \n",
    "            order by FL_DATE_MONTH asc\"\"\").show(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a0008b0-7626-4d34-9158-4fd41d3aa477",
   "metadata": {
    "id": "6a0008b0-7626-4d34-9158-4fd41d3aa477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+\n",
      "|OP_CARRIER|avg(T_REL_ELAP_TIME)|count(OP_CARRIER)|\n",
      "+----------+--------------------+-----------------+\n",
      "|        OH|  1.2796839370205886|             3495|\n",
      "|        MQ|  1.2766248194050211|             4861|\n",
      "|        OO|  1.2627619459473463|             9929|\n",
      "|        EV|  1.2457965593937206|             2767|\n",
      "|        9E|  1.2456329577493501|             2878|\n",
      "|        YV|  1.2440199282693694|             2619|\n",
      "|        YX|   1.218283853416388|             4304|\n",
      "|        WN|  1.2119350247324765|            15183|\n",
      "|        DL|  1.1882825925393505|            10262|\n",
      "|        G4|  1.1854409455457788|              987|\n",
      "+----------+--------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TOP 10 Carriers on avg delays\n",
    "df.filter(\"T_LATE = 1\").groupBy(\"OP_CARRIER\").agg({'OP_CARRIER':'count', 'T_REL_ELAP_TIME': 'avg'}).orderBy('avg(T_REL_ELAP_TIME)',ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4571a7e-13b7-4dae-8548-076c67829acb",
   "metadata": {
    "id": "f4571a7e-13b7-4dae-8548-076c67829acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+\n",
      "|ORIGIN|count(ORIGIN)|avg(T_REL_ELAP_TIME)|\n",
      "+------+-------------+--------------------+\n",
      "|   WRG|            4|  2.0115384615384615|\n",
      "|   PIB|            2|  1.9390243902439024|\n",
      "|   GST|            1|  1.8518518518518519|\n",
      "|   RHI|            1|  1.6764705882352942|\n",
      "|   AKN|            1|  1.6379310344827587|\n",
      "|   SIT|            8|  1.5736194691938201|\n",
      "|   SCC|            1|  1.5510204081632653|\n",
      "|   SHD|            4|  1.5334670231729055|\n",
      "|   OTZ|            8|  1.5026870443095257|\n",
      "|   CDV|            3|  1.4983530961791833|\n",
      "+------+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TOP 10 origin airports on avg delays\n",
    "df.filter(\"T_LATE = 1\").groupBy(\"ORIGIN\")\\\n",
    "  .agg({'ORIGIN':'count', 'T_REL_ELAP_TIME': 'avg'})\\\n",
    "  .orderBy('avg(T_REL_ELAP_TIME)',ascending=False)\\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18d28fb2-275d-40cf-8ff8-d493003ccf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|ORIGIN|DEST|\n",
      "+------+----+\n",
      "|   EWR| HNL|\n",
      "|   BWI| LAS|\n",
      "|   PHL| LAX|\n",
      "|   PDX| KOA|\n",
      "|   SEA| HNL|\n",
      "|   SEA| LIH|\n",
      "|   PDX| KOA|\n",
      "|   JFK| SEA|\n",
      "|   LGA| DAL|\n",
      "|   SEA| KOA|\n",
      "+------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('ORIGIN','DEST').show(10) ## RDD = 45 ms, cache = 32 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c57e6c9a-6ba2-4eef-8a2f-ec738148618e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019352"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() ## RDD = 9 s, cache = 9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49ceb0fd-d785-4f5a-a141-7b1039ed59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##caching my view\n",
    "df.repartition(1).createOrReplaceTempView('df')\n",
    "spark.catalog.cacheTable('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a4e20a6-47ab-4c32-896f-57b3373b19fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6f085-1ecd-4bc1-93dd-46316eb98af0",
   "metadata": {
    "id": "61c6f085-1ecd-4bc1-93dd-46316eb98af0",
    "tags": []
   },
   "source": [
    "# MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2df5ec75-cd11-4ab2-b490-6f22e7769a5f",
   "metadata": {
    "id": "2df5ec75-cd11-4ab2-b490-6f22e7769a5f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# hold out a small set of data for data predition\n",
    "# can also be used for reducing the nr of instances used for learning and testing\n",
    "# df, dfpred = df.randomSplit([0.1, 0.9], seed = 13) # Uncomment this line if you need a smaller dataset\n",
    "dftrain, dfpred = df.randomSplit([0.9, 0.1], seed = 13)\n",
    "\n",
    "# bluid ml pipeline for classif and reg\n",
    "#------------------------------\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator , RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# convert string attribs to categorical attribs\n",
    "string_attribs = [\"OP_CARRIER\",\n",
    "                  \"ORIGIN\",\n",
    "                  \"DEST\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=attrib_name, outputCol = attrib_name+\"_IDX\") for attrib_name in string_attribs]\n",
    "\n",
    "# group features\n",
    "feature_attribs = [\"FL_DATE_DAY_IN_WEEK\",\n",
    "                   \"FL_DATE_DAY_IN_MONTH\",\n",
    "                   \"FL_DATE_MONTH\",\n",
    "                   \"DEP_HOUR_IN_DAY\",\n",
    "                   \"REL_INIT_DELAY\",\n",
    "                   \"ARR_HOUR_IN_DAY\",\n",
    "                   \"OP_CARRIER_IDX\",\n",
    "                   \"ORIGIN_IDX\",\n",
    "                   \"DEST_IDX\",\n",
    "                   \"DISTANCE\",\n",
    "                   \"CRS_ELAPSED_TIME\"]\n",
    "\n",
    "\n",
    "label_attrib_clf = \"T_LATE\"\n",
    "label_attrib_reg = \"T_REL_ELAP_TIME\"\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_attribs, outputCol=\"features\")\n",
    "\n",
    "#estimators for classification and regression\n",
    "estimator_clf = DecisionTreeClassifier(featuresCol = \"features\", labelCol = label_attrib_clf)\n",
    "estimator_reg = DecisionTreeRegressor(labelCol = label_attrib_reg, featuresCol = \"features\")\n",
    "\n",
    "# parameters for estimators\n",
    "#TODO: apply final list of hyperparameters\n",
    "paramGrid_clf = ParamGridBuilder()\\\n",
    "                .addGrid(estimator_clf.maxBins, [360])\\\n",
    "                .addGrid(estimator_clf.seed, [13])\\\n",
    "                .addGrid(estimator_clf.impurity, [\"gini\", \"entropy\"])\\\n",
    "                .addGrid(estimator_clf.maxDepth, [*range(0, 31, 10)])\\\n",
    "                .addGrid(estimator_clf.minInstancesPerNode, [*range(1, 101, 25)])\\\n",
    "                .addGrid(estimator_clf.minInfoGain, [*np.arange(0, 1.1, 0.25)])\\\n",
    "                .addGrid(estimator_clf.minWeightFractionPerNode, [*np.arange(0, 0.49, 0.2)])\\\n",
    "                .build()\n",
    "\n",
    "paramGrid_reg = ParamGridBuilder()\\\n",
    "                .addGrid(estimator_reg.maxBins, [360])\\\n",
    "                .addGrid(estimator_reg.impurity, [\"variance\"])\\\n",
    "                .addGrid(estimator_reg.seed, [13])\\\n",
    "                .addGrid(estimator_reg.maxDepth, [0, 15])\\\n",
    "                .addGrid(estimator_reg.minInstancesPerNode, [1, 25, 50])\\\n",
    "                .addGrid(estimator_reg.minInfoGain, [0, 0.25, 0.50])\\\n",
    "                .addGrid(estimator_reg.minWeightFractionPerNode, [0, 0.2])\\\n",
    "                .build()\n",
    "\n",
    "# evaluators\n",
    "evaluator_clf = BinaryClassificationEvaluator(labelCol = label_attrib_clf,\n",
    "                                              metricName = 'areaUnderROC')\n",
    "evaluator_reg = RegressionEvaluator(labelCol = label_attrib_reg, \n",
    "                                    predictionCol = \"prediction\", \n",
    "                                    metricName = \"rmse\")\n",
    "\n",
    "# prep training\n",
    "tvs_clf = TrainValidationSplit(estimator = estimator_clf,\n",
    "                               estimatorParamMaps = paramGrid_clf,\n",
    "                               evaluator = evaluator_clf,\n",
    "                               trainRatio = 0.8,\n",
    "                               parallelism = 8,\n",
    "                               seed = 13)\n",
    "\n",
    "tvs_reg = TrainValidationSplit(estimator = estimator_reg,\n",
    "                               estimatorParamMaps = paramGrid_reg,\n",
    "                               evaluator = evaluator_reg,\n",
    "                               trainRatio = 0.8,\n",
    "                               parallelism = 8,\n",
    "                               seed = 13)\n",
    "\n",
    "# pipeline for transforming and training the data\n",
    "common_transf = Pipeline(stages = indexers + [assembler])\n",
    "stages_clf = [common_transf, tvs_clf]\n",
    "stages_reg = [common_transf, tvs_reg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8MzOHfQDhOOU",
   "metadata": {
    "id": "8MzOHfQDhOOU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier training completed\n",
      "Regressor training completed\n"
     ]
    }
   ],
   "source": [
    "# the result of the TVS goes here, i.e. the last element in stages (TrainValidationSplitModel)\n",
    "\n",
    "if autoExec:\n",
    "    mlExecReg = True\n",
    "    mlExecClf = True\n",
    "else: \n",
    "    mlExecReg = input('Train regressor (Y-yes, N-no)?: ').lower()=='y'\n",
    "    mlExecClf = input('Train classifier (Y-yes, N-no)?: ').lower()=='y'  \n",
    "\n",
    "if mlExecClf:\n",
    "    tvs_model_clf = Pipeline(stages=stages_clf).fit(dftrain).stages[-1]\n",
    "    print('Classifier training completed')\n",
    "\n",
    "if mlExecReg:\n",
    "    tvs_model_reg = Pipeline(stages=stages_reg).fit(dftrain).stages[-1]\n",
    "    print('Regressor training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba9e8b8c-e9a5-4b30-8ca9-89af71c0d86a",
   "metadata": {
    "id": "ba9e8b8c-e9a5-4b30-8ca9-89af71c0d86a"
   },
   "outputs": [],
   "source": [
    "from dtreeviz.models.spark_decision_tree import ShadowSparkTree\n",
    "from dtreeviz import trees\n",
    "\n",
    "# the best decision tree model goes here as a plot\n",
    "# https://explained.ai/decision-tree-viz/index.html\n",
    "# https://github.com/parrt/dtreeviz\n",
    "# https://github.com/parrt/dtreeviz/blob/master/notebooks/dtreeviz_spark_visualisations.ipynb\n",
    "\n",
    "\n",
    "if mlExecClf:\n",
    "    \n",
    "    if autoExec:\n",
    "        vizClf = False\n",
    "    else: \n",
    "        vizClf = input('Visualise Classifier (Y-yes, N-no)?').lower()=='y'   \n",
    "  \n",
    "    if vizClf:\n",
    "        dt_model_clf = tvs_model_clf.bestModel\n",
    "\n",
    "        viz_dataset = Pipeline(stages=[common_transf])\\\n",
    "             .fit(dftrain)\\\n",
    "             .transform(dftrain)\\\n",
    "             .toPandas()[feature_attribs + [label_attrib_clf,label_attrib_reg]]\n",
    "\n",
    "        dt_clf = ShadowSparkTree(dt_model_clf, \n",
    "                                  viz_dataset[feature_attribs], \n",
    "                                  viz_dataset[label_attrib_clf], \n",
    "                                  feature_names = feature_attribs, \n",
    "                                  target_name = label_attrib_clf, \n",
    "                                  class_names=[0, 1])\n",
    "\n",
    "        display(trees.dtreeviz(dt_clf, fancy=True, fontname = \"DejaVu Sans\"))\n",
    "        \n",
    "if mlExecReg:\n",
    "    if autoExec:\n",
    "        vizReg = False\n",
    "    else: \n",
    "        vizReg = input('Visualise Regresspr (Y-yes, N-no)?').lower()=='y'    \n",
    "    \n",
    "    if vizReg:\n",
    "        dt_model_reg = tvs_model_reg.bestModel\n",
    "        \n",
    "        dt_reg = ShadowSparkTree(dt_model_reg, \n",
    "                                  viz_dataset[feature_attribs], \n",
    "                                  viz_dataset[label_attrib_reg], \n",
    "                                  feature_names = feature_attribs, \n",
    "                                  target_name = label_attrib_reg)\n",
    "\n",
    "        display(trees.dtreeviz(dt_reg, fancy=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b0e28-13b3-446d-aaaa-1d5e88af4dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94c33852-2ad3-4c44-bb48-f9ad63927be8",
   "metadata": {
    "id": "94c33852-2ad3-4c44-bb48-f9ad63927be8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics:\n",
      "Classifier metrics (areaUnderROC) :  0.598\n",
      "Regressor metrics (rmse) : 0.093\n"
     ]
    }
   ],
   "source": [
    "#the best decision tree model go here\n",
    "#tvs_model_clf.bestModel.toDebugString\n",
    "#tvs_model_reg.bestModel.toDebugString\n",
    "\n",
    "#best models performance\n",
    "print(\"Performance metrics:\")\n",
    "if mlExecClf:\n",
    "    print(f\"Classifier metrics ({evaluator_clf.getMetricName()}) : \", round(max(tvs_model_clf.validationMetrics), 3))\n",
    "if mlExecReg:\n",
    "    print(f\"Regressor metrics ({evaluator_reg.getMetricName()}) :\", round(min(tvs_model_reg.validationMetrics), 3))\n",
    "\n",
    "#model hyperparameters and metrics report\n",
    "#tvs_model_clf.write().overwrite().save(\"./model_clf\")\n",
    "#tvs_model_reg.write().overwrite().save(\"./model_reg\")\n",
    "\n",
    "#predictions on unseen data\n",
    "#predictions = Pipeline(stages=stages_clf).fit(dftrain).transform(dfpred)\n",
    "#predictions.select(\"prediction\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7db60ea7-39eb-42bd-940c-8f70a17a092b",
   "metadata": {
    "id": "7db60ea7-39eb-42bd-940c-8f70a17a092b"
   },
   "outputs": [],
   "source": [
    "# Stops the process\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ALL_08051723.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
