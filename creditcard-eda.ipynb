{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Credit Card Fraud\n",
    "\n",
    "Fraud activities are considered uncommon or outliers transactions, which is probably one of the main characteristics regarding Fraud. As the authors of the book Fraud Analytics using Descriptive, Predictive, and Social Network Techniques: A Guide to Data Science for Fraud Detection pointed out:\n",
    "\n",
    "“This makes it difficult [because of the outlier characteristic] to both detect fraud, since the fraudulent cases are covered by the nonfraudulent ones, as well as to learn from historical cases to build a powerful fraud-detection system since only few examples are available”\n",
    "\n",
    "Hence, it is imperative to overcome this issue considering the unbalanced nature of the data.\n",
    "\n",
    "In this project, we will use the credit card data available in [https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud). Moreover, we will use PySpark, an Interface for Apache Spark in Python, since it is an excellent tool dealing with Big Data.\n",
    "\n",
    "### Feature Technicalities:\n",
    "\n",
    "- PCA Transformation: The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) (Except for time and amount).\n",
    "- Scaling: In order to implement a PCA transformation features need to be previously scaled. (In this case, all the V features have been scaled)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA - Exploratory Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import PySpark libraries\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LinearSVC, NaiveBayes\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Configure Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Credit Card Fraud\") \\\n",
    "    .config(\"spark.master\", \"local\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T08:56:00.916538Z",
     "end_time": "2023-04-30T08:56:00.973299Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load file\n",
    "df = spark.read.csv('datasets/creditcard.csv', header=True, inferSchema=True, sep=\",\")\n",
    "# Print Schema\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T23:52:05.700988Z",
     "end_time": "2023-04-29T23:52:15.288511Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Spark DataFrame Shape\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "print(f\"Number of Records: {df.count()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T23:54:36.541327Z",
     "end_time": "2023-04-29T23:54:37.915127Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset contains **284,807 records** and consists of **31 columns**. Out of these columns, **28 columns (V1 to V28)** are results of the **Principal Component Analysis (PCA)** technique. These columns are likely to be more abstract in nature since they are generated from linear combinations of the original features. Therefore, it may be difficult to interpret the individual contributions of these features towards the prediction task.\n",
    "\n",
    "On the other hand, the remaining three columns are:\n",
    "\n",
    "- **Time**: This column indicates the time elapsed in seconds between each transaction and the first transaction in the dataset.\n",
    "- **Amount**: This column indicates the transaction amount.\n",
    "- **Class**: This column indicates the fraud status of the transaction. The value 1 indicates fraud, and 0 indicates a legitimate transaction.\n",
    "\n",
    "It is important to note that since the **PCA** technique is used, the original feature names and descriptions are not available. Therefore, feature engineering might be necessary to extract meaningful features from the given dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspecting the first 10 records\n",
    "df.limit(10).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T23:56:44.752675Z",
     "end_time": "2023-04-29T23:56:45.214888Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#statistics\n",
    "df.toPandas().describe().T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:01:51.570377Z",
     "end_time": "2023-04-30T00:01:58.666804Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset contains a total of **284807 records**. The **time column** in the dataset indicates the time elapsed in seconds between each transaction and the first transaction in the dataset. The maximum time recorded between transactions is **172,792 seconds**. This is equivalent to 2880 minutes or approximately **48 hours**. Since the timeframe of the dataset is only 2 days, it means that the last transaction was made two days after the first one.\n",
    "\n",
    "The amount column indicates the transaction amount. The biggest transaction recorded in the dataset is worth **$25,691.16**.\n",
    "\n",
    "\n",
    "## Checking Missing Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Check missing and null data\n",
    "df.select([f.count(f.when(f.isnan(c) | f.col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:06:48.080214Z",
     "end_time": "2023-04-30T00:06:52.356408Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upon checking the dataset, it was observed that there are no missing values present.\n",
    "Each column was inspected for null or NaN values, and it was found that the dataset is complete!\n",
    "\n",
    "\n",
    "## 1.2 Univariate Analysis\n",
    "\n",
    "### 1.2.1 Class\n",
    "\n",
    "#### Q1. HOW MANY ROWS IN THE DATASET REPRESENT CREDIT CARD FRAUD?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Number of Frauds and non-frauds\n",
    "classFreq = df.groupBy(\"Class\").count()\n",
    "classFreq.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:09:48.132209Z",
     "end_time": "2023-04-30T00:09:50.091811Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total = classFreq.select(\"count\").agg({\"count\": \"sum\"}).collect().pop()['sum(count)']\n",
    "result = classFreq.withColumn('percent', f.format_number(classFreq['count']/total * 100, 2))\n",
    "result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:12:03.510675Z",
     "end_time": "2023-04-30T00:12:06.220339Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Out of the total of **284,807 records**, only **492** represent credit card fraud, which is approximately **0.17%** of the dataset. Therefore, we have an imbalanced dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a pandas dataframe with only the Class column\n",
    "fraud_df = df.select(\"Class\").toPandas()\n",
    "\n",
    "# Count the number of transactions for each class (fraudulent or not)\n",
    "fraud_counts = fraud_df['Class'].value_counts()\n",
    "\n",
    "# Calculate the percentage and absolute number of each class\n",
    "fraud_perc = fraud_counts / fraud_counts.sum() * 100\n",
    "fraud_abs = fraud_counts.values\n",
    "\n",
    "# Create a bar plot with logarithmic y-axis\n",
    "sns.barplot(x=fraud_counts.index, y=fraud_counts.values)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Add percentage and absolute value labels to the bars\n",
    "for i, v in enumerate(fraud_counts):\n",
    "    plt.text(i, v, f\"{fraud_perc[i]:.1f}% ({fraud_abs[i]:,})\", ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Set the y-axis labels and formatter\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: '{:.0f} K'.format(x/1000)))\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel('Transaction Class')\n",
    "plt.xticks([0, 1], ['Non-Fraud', 'Fraud'])\n",
    "plt.title('Distribution of fraudulent transactions')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:15:17.636462Z",
     "end_time": "2023-04-30T00:15:19.734681Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned earlier, the majority of transactions are legitimate. If we use this dataset as the foundation for our predictive models and analysis, we may encounter many errors, and our algorithms are likely to overfit since they will \"presume\" that most transactions are not fraudulent. However, we do not want our model to presume; we want our model to identify patterns that indicate fraud.\n",
    "\n",
    "\n",
    "\n",
    "### 1.2.2 Amount -- Should we remove this? At least for me, doesnt make much sense."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a function to group time values\n",
    "@f.udf(returnType=t.StringType())\n",
    "def time_udf(time):\n",
    "    if time < 50000:\n",
    "        return \"Under 50K s\"\n",
    "    elif 50000 <= time <= 100000:\n",
    "        return \"Between 50K and 100K s\"\n",
    "    elif time > 100000:\n",
    "        return \"Over 100K s\"\n",
    "    else:\n",
    "        return \"NA\"\n",
    "\n",
    "# apply the function to the \"Time\" column\n",
    "df = df.withColumn('time_udf', time_udf('Time'))\n",
    "\n",
    "df.limit(10).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:24:40.089339Z",
     "end_time": "2023-04-30T00:24:42.937067Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define window function with partitionBy clause\n",
    "window = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "# Create time group table\n",
    "time_group_table = df.select(\"time_udf\", \"Amount\"). \\\n",
    "    groupBy(\"time_udf\"). \\\n",
    "    agg(\n",
    "    f.count(\"Amount\").alias(\"UserCount\"),\n",
    "    f.mean(\"Amount\").alias(\"Amount_Avg\"),\n",
    "    f.min(\"Amount\").alias(\"Amount_Min\"),\n",
    "    f.max(\"Amount\").alias(\"Amount_Max\")). \\\n",
    "    withColumn(\"total\", f.sum(\"UserCount\").over(window)). \\\n",
    "    withColumn(\"Percent\", f.round(f.col(\"UserCount\")*100 / f.col(\"total\"), 2)). \\\n",
    "    drop(\"total\"). \\\n",
    "    orderBy(f.desc(\"Percent\"))\n",
    "\n",
    "time_group_table.limit(10).toPandas()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:35:31.653136Z",
     "end_time": "2023-04-30T00:35:33.924863Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert PySpark dataframe to Pandas dataframe\n",
    "time_group_df = time_group_table.toPandas()\n",
    "\n",
    "# Create barplot\n",
    "ax = sns.barplot(x=\"time_udf\", y=\"Percent\", data=time_group_df)\n",
    "\n",
    "for p in ax.patches:\n",
    "    abs_value = p.get_height()\n",
    "    ax.annotate(f'{abs_value}',\n",
    "                (p.get_x() + p.get_width() / 2., abs_value),\n",
    "                ha='center', va='center',\n",
    "                xytext=(0, 9),\n",
    "                textcoords='offset points')\n",
    "\n",
    "plt.title(\"User Count and Transaction Amount by Time Group\")\n",
    "plt.xlabel(\"Time Group\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.show();"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:38:49.563788Z",
     "end_time": "2023-04-30T00:38:50.196215Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# All Transactions\n",
    "df_aux = df.select(\"Class\", \"Amount\").toPandas()\n",
    "\n",
    "# Define amount ranges\n",
    "amount_ranges = [\n",
    "    {\"range\": \"Transaction Value <= $100\", \"min_amount\": 0, \"max_amount\": 100},\n",
    "    {\"range\": \"Transaction Value between \\$101 and \\$2000\", \"min_amount\": 101, \"max_amount\": 2000},\n",
    "    {\"range\": \"Transaction Value between \\$2001 and \\$5000\", \"min_amount\": 2001, \"max_amount\": 5000},\n",
    "    {\"range\": \"Transaction Value > $5000\", \"min_amount\": 5001, \"max_amount\": df_aux[\"Amount\"].max()}\n",
    "]\n",
    "\n",
    "# Create four subplots for different amount ranges\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "for idx, amount_range in enumerate(amount_ranges):\n",
    "    row_idx = idx // 2\n",
    "    col_idx = idx % 2\n",
    "\n",
    "    # Filter transactions within amount range\n",
    "    df_range = df_aux[(df_aux[\"Amount\"] > amount_range[\"min_amount\"]) & (df_aux[\"Amount\"] <= amount_range[\"max_amount\"])]\n",
    "\n",
    "    # Plot histogram\n",
    "    sns.histplot(data=df_range, x=\"Amount\", ax=axes[row_idx, col_idx], hue=\"Class\", kde=True)\n",
    "    axes[row_idx, col_idx].set_title(amount_range[\"range\"])\n",
    "    axes[row_idx, col_idx].set_ylabel(\"Number of Transactions\")\n",
    "    axes[row_idx, col_idx].legend(labels=[\"Fraud\", \"Non-Fraud\"])\n",
    "\n",
    "fig.suptitle(\"All Transactions\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:40:40.027113Z",
     "end_time": "2023-04-30T00:40:44.478063Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Only fraud transactions\n",
    "only_fraud = df.filter(df.Class == 1).select(\"Amount\").toPandas()\n",
    "\n",
    "# Add a pallet\n",
    "aux_pal = [\"#ff7f7f\", \"#ff3c3c\"]\n",
    "\n",
    "# Create three subplots for different amount ranges\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "# Amount range <= 100\n",
    "sns.histplot(data=only_fraud[only_fraud[\"Amount\"] <= 100], x=\"Amount\", ax=axes[0], kde=True, color=aux_pal[0])\n",
    "axes[0].set_title(\"Transaction Amount <= $100\")\n",
    "axes[0].set_ylabel(\"Number of Transactions\")\n",
    "\n",
    "# Amount range > 100\n",
    "sns.histplot(data=only_fraud[(only_fraud[\"Amount\"] > 100)], x=\"Amount\", ax=axes[1], kde=True, color=aux_pal[1])\n",
    "axes[1].set_title(\"Transaction Amount > $100\")\n",
    "axes[1].set_ylabel(\"Number of Transactions\")\n",
    "\n",
    "fig.suptitle(\"Only Fraudulent Transactions\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:41:04.178356Z",
     "end_time": "2023-04-30T00:41:06.053233Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2.3 Remaining variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create several histograms to show the distributions of the features\n",
    "fig, axs = plt.subplots(8, 4, figsize=(25, 15))\n",
    "fig.suptitle(\"Distribution of Features\", fontsize=14)\n",
    "\n",
    "for col, ax in zip(df.columns, axs.flatten()):\n",
    "    ax.hist(df.select(col).toPandas()[col])\n",
    "    ax.grid(False)\n",
    "    ax.tick_params(axis='x', labelrotation=45, labelsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=15)\n",
    "    ax.set_title(col.upper(), fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9, hspace=0.4)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:42:16.924178Z",
     "end_time": "2023-04-30T00:43:18.843565Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Through the histograms, we can observe that the feature \"Time\" does not follow a normal distribution, with a lower occurrence in the range of 75,000-100,000 seconds. On the other hand, the features Vx show a normalized distribution centered around the value 0, with some negative values.\n",
    "\n",
    "## 1.3 Imbalanced Data\n",
    "\n",
    "To address the imbalanced data issue, we will implement the technique of Random Under Sampling. The aim is to balance the dataset by removing data, which helps to avoid model overfitting.\n",
    "\n",
    "The following steps will be taken:\n",
    "\n",
    "    - Determine the degree of imbalance in the class by using the \"value_counts()\" function on the class column to count the number of instances for each label.\n",
    "\n",
    "    - Bring the number of non-fraud transactions to the same amount as fraud transactions (assuming we want a 50/50 ratio),which will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.\n",
    "\n",
    "    - Shuffle the data to ensure that our models can maintain a certain accuracy every time we run the script.\n",
    "\n",
    "**Note**: It is important to note that the Random Under Sampling technique may result in a loss of information since we are reducing the number of instances in the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select fraud and non-fraud transactions and limit non-fraud transactions to the same number as fraud transactions\n",
    "fraud_df = df.filter(f.col('Class') == 1)\n",
    "non_fraud_df = df.filter(f.col('Class') == 0).limit(fraud_df.count())\n",
    "\n",
    "# Combine fraud and non-fraud transactions and shuffle the data\n",
    "balanced_df = fraud_df.union(non_fraud_df).orderBy(f.rand())\n",
    "\n",
    "# Show 10 rows of the shuffled, balanced dataframe\n",
    "balanced_df.limit(10).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:47:00.874878Z",
     "end_time": "2023-04-30T00:47:07.369452Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert the Spark dataframe to a pandas dataframe\n",
    "fraud_df = balanced_df.select(\"Class\").toPandas()\n",
    "\n",
    "# Create a countplot with logarithmic y-axis\n",
    "sns.countplot(x='Class', data=fraud_df, palette='Set1', order=[1, 0])\n",
    "\n",
    "# Calculate the percentage of each class\n",
    "fraud_counts = fraud_df['Class'].value_counts()\n",
    "fraud_perc = fraud_counts / fraud_counts.sum() * 100\n",
    "\n",
    "# Set the y-axis labels and formatter\n",
    "plt.ylabel('Number of Transactions')\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel('Transaction Class')\n",
    "plt.xticks([0, 1], ['Fraud', 'Non-Fraud'])\n",
    "plt.title('Balanced Distribution of Transactions')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T08:46:14.605477Z",
     "end_time": "2023-04-30T08:46:17.533215Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have our dataframe correctly balanced, we can go further with our analysis and data preprocessing.\n",
    "\n",
    "\n",
    "## Correlations\n",
    "\n",
    "Correlation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (Balanced) in order for us to see which features have a high positive or negative correlation in regard to fraud transactions.\n",
    "\n",
    "Summary and Explanation:\n",
    "\n",
    "    Negative Correlations: V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.\n",
    "    Positive Correlations: V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction.\n",
    "    BoxPlots: We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions.\n",
    "\n",
    "Note: We have to make sure we use the subsample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original dataframe.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\"time_udf\")\n",
    "balanced_df = balanced_df.drop(\"time_udf\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(df.toPandas().corr(), cmap='coolwarm_r')\n",
    "plt.title('Correlations with Unbalanced Data')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(balanced_df.toPandas().corr(), cmap='coolwarm_r')\n",
    "plt.title('Correlations with Balanced Data')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:53:53.414505Z",
     "end_time": "2023-04-30T00:54:01.986343Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(balanced_df.toPandas().corr() > 0.7,cbar=False, annot_kws={'size':20})\n",
    "plt.title('Correlations with Balanced Data > 0.7')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(balanced_df.toPandas().corr() < -0.7,cbar=False, annot_kws={'size':20})\n",
    "plt.title('Correlations with Balanced Data < -0.7')\n",
    "\n",
    "plt.show();"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:52:56.620180Z",
     "end_time": "2023-04-30T00:52:58.343689Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Conclusions from Correlation Matrix\n",
    "Based on our correlation matrix, we have concluded the following:\n",
    "\n",
    "- The variables that were a product of PCA are not correlated with each other, meaning they are independent.\n",
    "- The variable \"Time\" seems to be negatively correlated with all of the \"Vx\" variables, which means that as \"Time\" increases, the values of \"Vx\" decrease, and vice versa.\n",
    "- The variables \"Time\" and \"Class\" seem to have no correlation, indicating that the time at which a transaction occurs has no influence on whether it is fraudulent or not.\n",
    "- The variable \"Class\" seems to be negatively correlated with some of the \"Vx\" variables, indicating that certain values of \"Vx\" are more likely to be associated with fraudulent transactions, while not correlated at all with others.\n",
    "\n",
    "\n",
    "## Distributions: Univariate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select all columns except the ones to exclude\n",
    "cols_to_include = [col for col in balanced_df.columns if col not in ['Time']]\n",
    "df_to_plot = balanced_df.select(*cols_to_include)\n",
    "\n",
    "for col in df_to_plot.columns:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    # plot histogram\n",
    "    axs[0].hist(df_to_plot.select(col).rdd.flatMap(lambda x: x).collect(), bins=50)\n",
    "    axs[0].set_title(f\"Distribution of {col} values\", size=14)\n",
    "    axs[0].set_xlabel(col, size=12)\n",
    "    axs[0].set_ylabel(\"Count\", size=12)\n",
    "\n",
    "    # plot boxplot\n",
    "    sns.boxplot(data=df_to_plot.select(col).toPandas(), x=col, ax=axs[1])\n",
    "    axs[1].set_title(f\"{col} Boxplot\", size=14)\n",
    "    axs[1].set_xlabel(col, size=12)\n",
    "    axs[1].set_ylabel(\"Count\", size=12)\n",
    "\n",
    "plt.show();"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:57:58.174475Z",
     "end_time": "2023-04-30T01:01:17.929222Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distributions: Bivariate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bivariate Analysis\n",
    "\n",
    "data = df_to_plot.toPandas()\n",
    "fig, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
    "\n",
    "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=data,  ax=axes[0])\n",
    "axes[0].set_title('V14 vs Class Negative Correlation')\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V12\", data=data,  ax=axes[1])\n",
    "axes[1].set_title('V12 vs Class Negative Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V10\", data=data, ax=axes[2])\n",
    "axes[2].set_title('V10 vs Class Negative Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V16\", data=data, ax=axes[3])\n",
    "axes[3].set_title('V16 vs Class Negative Correlation')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T08:55:54.487960Z",
     "end_time": "2023-04-30T08:55:55.691778Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
    "\n",
    "# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\n",
    "sns.boxplot(x=\"Class\", y=\"V4\", data=data,  ax=axes[0])\n",
    "axes[0].set_title('V4 vs Class Positive Correlation')\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V11\", data=data,  ax=axes[1])\n",
    "axes[1].set_title('V11 vs Class Positive Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V2\", data=data, ax=axes[2])\n",
    "axes[2].set_title('V2 vs Class Positive Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V19\", data=data, ax=axes[3])\n",
    "axes[3].set_title('V19 vs Class Positive Correlation')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T08:50:18.783778Z",
     "end_time": "2023-04-30T08:50:19.264578Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hist_fraud = df_to_plot.filter(f.col('Class') == 1).drop('Class').toPandas()\n",
    "\n",
    "hist_fraud.hist(bins=30, figsize=(10, 10))\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1,\n",
    "                    right=0.9,\n",
    "                    top=0.9,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.8)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T09:00:33.163128Z",
     "end_time": "2023-04-30T09:00:42.356482Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
