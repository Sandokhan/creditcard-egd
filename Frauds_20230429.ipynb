{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ea7516-dca3-4fac-80dd-072b008fa5bf",
   "metadata": {},
   "source": [
    "## CREDIT CARD FRAUD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709dbfad-80a1-4a27-9342-97bd651de874",
   "metadata": {},
   "source": [
    "## 1.0 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e356c9a-d325-4803-8656-e3ea83c9609b",
   "metadata": {},
   "source": [
    "Firstly we need to import the necessary libraries and configure the Spark session. After that, we load our data set, in order to analyse it, and perform EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488381a-9bd0-429f-bfb2-339f31d74138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import libraries and configure spark session\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"xpto\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e7cfb-0c94-49d8-97fc-2c6bd11d2033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load file\n",
    "df = spark.read.csv('creditcard.csv', header=True, inferSchema=True, sep=\",\")\n",
    "\n",
    "# Print Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56297f7a-1d76-4a22-bf48-8d6dc9d4ba88",
   "metadata": {},
   "source": [
    "It appears that the dataset has a total of 31 columns, in which 28 of them are components that are results of PCA. Columns V1-V28 are going to be more abstract, since we don't have a clear definition/description of what they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d881d-bd82-4b51-9f64-bba24ff2bb09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca5cb3-848e-4ba9-be7f-496910f6ffed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Shape of the Dataframe\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eade08c7-b8fd-489a-a47e-b4977b82ca94",
   "metadata": {
    "tags": []
   },
   "source": [
    "After performing the counts above, we conclude that we are going to be working with a dataset that has 284 807 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8f027-70d3-422a-b6f4-f6c2a7cbbd34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.toPandas().info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd21ce8-e05d-419b-8272-39ab83e99ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09678663-6137-42f0-b1e8-da3202f73e39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#statistics\n",
    "df.toPandas().describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9106d1-4c4e-4243-b4d4-6c8c44ecb89b",
   "metadata": {},
   "source": [
    "The maximum amount of time (in seconds) recorded between transactions is 172792 seconds, which is equivalent to 2880 minutes, or approx. 48 hours. We can also conclude that the maximum value for Amount is $ 25691.16."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11221b4-3124-45bd-841e-9819237546d8",
   "metadata": {},
   "source": [
    "## 1.1 CHECKING MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7e5ae-dc58-44dd-bf68-083633ea2f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check missing and null data\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae169c0f-bdb5-487b-b24f-51b45dcead34",
   "metadata": {},
   "source": [
    "Above checked for missing values. Each column shows the amount of null or NaN values in each column. It appears that the dataset doesn't have missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8fee41-39fd-4969-94b5-c2ed203cc8dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HOW MANY ROWS IN THE DATASET REPRESENT CREDIT CARD FRAUD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880e5e42-2cc8-4c54-9836-784d97411577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Number of Frauds and non-frauds\n",
    "classFreq = df.groupBy(\"Class\").count()\n",
    "classFreq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b41fd-c288-48da-96d9-7d19135def8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total = classFreq.select(\"count\").agg({\"count\": \"sum\"}).collect().pop()['sum(count)']\n",
    "result = classFreq.withColumn('percent', (classFreq['count']/total) * 100)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c780c248-4b1b-4ccd-9029-66dc5af6958a",
   "metadata": {},
   "source": [
    "Here we can see that, out of the 284 807 rows of information available, 492 represent credit card fraud, which translates to approx. 17% of the records. With these results, it is safe to say that we have an unbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c34bfe-6838-4244-8fbc-7fcce84d8275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fraud_df = df.select(\"Class\").toPandas()\n",
    "\n",
    "# Count the number of transactions for each class (fraudulent or not)\n",
    "fraud_counts = fraud_df['Class'].value_counts()\n",
    "\n",
    "# Calculate the percentage of each class\n",
    "fraud_perc = fraud_counts / fraud_counts.sum() * 100\n",
    "\n",
    "# Create a bar plot with logarithmic y-axis\n",
    "sns.barplot(x=fraud_counts.index, y=fraud_counts.values)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Add percentage labels to the bars\n",
    "for i, v in enumerate(fraud_counts):\n",
    "    plt.text(i, v, f\"{fraud_perc[i]:.1f}%\", ha='center', fontweight='bold')\n",
    "\n",
    "# Set the y-axis labels and formatter\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: '{:.0f} K'.format(x/1000)))\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel('Transaction Class')\n",
    "plt.xticks([0, 1], ['Non-Fraud', 'Fraud'])\n",
    "plt.title('Distribution of fraudulent transactions')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e2942-6d33-41af-bcb9-d99210792c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create several histograms to show the distributions of the features\n",
    "fig = plt.figure(figsize=(25, 15))\n",
    "subtitle = fig.suptitle(\"Distribution of Features\", fontsize=14, verticalalignment=\"center\")\n",
    "for col, num in zip(df.toPandas().describe().columns, range(1, 11)):\n",
    "    ax = fig.add_subplot(3, 4, num)\n",
    "    ax.hist(df.toPandas()[col])\n",
    "    plt.grid(False)\n",
    "    plt.xticks(rotation=45, fontsize=14)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.title(col.upper(), fontsize=14)\n",
    "plt.tight_layout()\n",
    "subtitle.set_y(0.95)\n",
    "fig.subplots_adjust(top=0.85, hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64526a09-4be2-4ba3-9dd1-8d745f8d339d",
   "metadata": {},
   "source": [
    "By analysing these plots, we conclude that Time does not have a normal distribution and the values 75 000-100 000 seconds are the ones with less occurences throughout the dataset. All of the Vx componentes are normalized, and with most occurences centered around the value 0. These feautures also show some negative values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9ccd3-986c-46e5-b9a1-da0182f37794",
   "metadata": {
    "tags": []
   },
   "source": [
    "## USER DEFINED FUCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6665f3-dc93-4c30-a52d-b4e21a8cddb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Firstly lets create YES or NO values to assign to the values in our \"Class\" column. If the values is 1, then we assign it a \"yes\", and if the value is 0, then we assign it a \"no\". After developing the function, we also create a column called \"IsFraud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a6e07-4d24-4744-94a6-cfde3e0facc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the yes/no function\n",
    "y_udf = f.udf(lambda y: \"no\" if y == 0 else \"yes\", f.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e8dd6-868c-4beb-b36f-60282e368dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the new column for the yes/no values\n",
    "df = df.withColumn(\"IsFraud\", y_udf('Class'))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc38bac-e962-40f2-90d2-907f3d850d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a function to group time values\n",
    "def udf_multi(time):\n",
    "    if(time < 50000):\n",
    "        return \"Under 50K s\"\n",
    "    elif(time >= 50000 and time <= 100000):\n",
    "        return \"Between 50K and 100K s\"\n",
    "    elif(time > 100000):\n",
    "        return \"Over 100K s\"\n",
    "    else: return \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a6d81-2c02-4471-a7b9-bf684325fef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply the function to the \"Time column\"\n",
    "time_udf = f.udf(udf_multi)\n",
    "df = df.withColumn('time_udf', time_udf('Time'))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5376cf4-bf4c-430c-9c65-38d1ba292117",
   "metadata": {},
   "source": [
    "APPLYING SOME STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca484eea-b88a-4b8a-80d0-38bfb2599398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window \n",
    "window = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86683eec-27a2-4b6a-8f18-1ef2f0a01dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lets create a table\n",
    "time_group_table = df.select([\"time_udf\", \"Amount\"]).\\\n",
    "                         groupBy('time_udf').\\\n",
    "                            agg(\n",
    "                                f.count(\"Amount\").alias(\"UserCount\"),\n",
    "                                f.mean(\"Amount\").alias(\"Amount_Avg\"),\n",
    "                                f.min(\"Amount\").alias(\"Amount_Min\"),\n",
    "                                f.max(\"Amount\").alias(\"Amount_Max\")).\\\n",
    "                            withColumn(\"total\", f.sum(f.col(\"UserCount\")).over(window)).\\\n",
    "                            withColumn(\"Percent\", f.col(\"UserCount\")*100 / f.col(\"total\")).\\\n",
    "                            drop(f.col(\"total\")).sort(f.desc(\"Percent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6b17d-ba42-4ac2-b858-6300c394114d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_group_table.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4cbe7-6db4-4ae6-b1b5-fcf8184a0a46",
   "metadata": {},
   "source": [
    "Here we computed some statistics with the \"Amount\" column, divided by the three time groups defined earlier. Now let's plot our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2264f22a-7c2c-4516-b4e2-475fc5986ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=\"time_udf\", y=\"Percent\", data=time_group_table.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0422d-7947-493e-86d2-43049cb92aa2",
   "metadata": {},
   "source": [
    "After analysing the graph, we conclude that the largest percentage for the Amount values is found when the time between each transaction recorded is over 100 000 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e992a9-b488-4ca4-9648-b8312489f2d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All Transactions\n",
    "df_aux = df.select(\"Class\", \"Amount\").toPandas()\n",
    "\n",
    "# Define amount ranges\n",
    "amount_ranges = [\n",
    "    {\"range\": \"Transaction Value <= $100\", \"min_amount\": 0, \"max_amount\": 100},\n",
    "    {\"range\": \"Transaction Value between \\$101 and \\$2000\", \"min_amount\": 101, \"max_amount\": 2000},\n",
    "    {\"range\": \"Transaction Value between \\$2001 and \\$5000\", \"min_amount\": 2001, \"max_amount\": 5000},\n",
    "    {\"range\": \"Transaction Value > $5000\", \"min_amount\": 5001, \"max_amount\": df_aux[\"Amount\"].max()}\n",
    "]\n",
    "\n",
    "# Create four subplots for different amount ranges\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "for idx, amount_range in enumerate(amount_ranges):\n",
    "    row_idx = idx // 2\n",
    "    col_idx = idx % 2\n",
    "\n",
    "    # Filter transactions within amount range\n",
    "    df_range = df_aux[(df_aux[\"Amount\"] > amount_range[\"min_amount\"]) & (df_aux[\"Amount\"] <= amount_range[\"max_amount\"])]\n",
    "\n",
    "    # Plot histogram\n",
    "    sns.histplot(data=df_range, x=\"Amount\", ax=axes[row_idx, col_idx], hue=\"Class\", kde=True)\n",
    "    axes[row_idx, col_idx].set_title(amount_range[\"range\"])\n",
    "    axes[row_idx, col_idx].set_ylabel(\"Number of Transactions\")\n",
    "    axes[row_idx, col_idx].legend(labels=[\"Fraud\", \"Non-Fraud\"])\n",
    "\n",
    "fig.suptitle(\"All Transactions\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4cc52b-25fd-45e7-977d-ded02984de9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only fraud transactions\n",
    "only_fraud = df.filter(df.Class == 1).select(\"Amount\").toPandas()\n",
    "\n",
    "# Add a pallet\n",
    "aux_pal = [\"#ff7f7f\", \"#ff3c3c\"]\n",
    "\n",
    "# Create three subplots for different amount ranges\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "# Amount range <= 100\n",
    "sns.histplot(data=only_fraud[only_fraud[\"Amount\"] <= 100], x=\"Amount\", ax=axes[0], kde=True, color=aux_pal[0])\n",
    "axes[0].set_title(\"Transaction Amount <= $100\")\n",
    "axes[0].set_ylabel(\"Number of Transactions\")\n",
    "\n",
    "# Amount range > 100\n",
    "sns.histplot(data=only_fraud[(only_fraud[\"Amount\"] > 100)], x=\"Amount\", ax=axes[1], kde=True, color=aux_pal[1])\n",
    "axes[1].set_title(\"Transaction Amount > $100\")\n",
    "axes[1].set_ylabel(\"Number of Transactions\")\n",
    "\n",
    "fig.suptitle(\"Only Fraudulent Transactions\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d8937c-1398-45f6-84e5-b1c0619bc9e8",
   "metadata": {},
   "source": [
    "Now we move on and try to do some correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0fca0-371d-4699-a1dd-19474e7eee84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# create a dataframe with only numeric features in order to simplify our correlactions\n",
    "numeric_features = [t[0] for t in df.dtypes if t[1] != 'string']\n",
    "numeric_features_df = df.select(numeric_features)\n",
    "numeric_features_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2203d-846a-4f15-9afd-364b64453cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "col_names = numeric_features_df.columns\n",
    "features = numeric_features_df.rdd.map(lambda row: row[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db6a18-8a56-4685-bdff-a500a4c40270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import Statistics from mllib -> Pyspark and import Pandas\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ed113-80d1-4a94-96a0-8a054b1e4aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a correlation matrix\n",
    "corr_matrix = Statistics.corr(features, method=\"pearson\")\n",
    "corr_df = pd.DataFrame(corr_matrix)\n",
    "corr_df.index = col_names\n",
    "corr_df.columns = col_names\n",
    "round(corr_df, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa6883-17a7-4b2f-a931-934b5aec77d5",
   "metadata": {},
   "source": [
    "Since the dataset has a lot of information, it is best to do a heatmap, to facilitate visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a94b5-088a-4eec-aabe-92b5ed4e3663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the heatmap\n",
    "sns.heatmap(corr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91a5cf-09ad-4642-8e90-32dc160b4275",
   "metadata": {},
   "source": [
    "By looking at our correlation matrix, we arrived at the following conclusions:\n",
    "\n",
    "    The variables that were a product of PCA are not correlated with each other;\n",
    "    \"Time\" seems to be negatively correlated with all of the \"Vx\" variables;\n",
    "    \"Time\" and \"Class\" seem to have no correlation;\n",
    "    \"Class\" seem to be negatively correlated with some of the \"Vx\" variables, and not correlated at all with others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f8cf0-1412-4996-8194-3f7e46f18a24",
   "metadata": {},
   "source": [
    "## 1.2 IMBALANCED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4783d-314e-43db-9b24-d3b1de66f5f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1= df.toPandas()\n",
    "df1= df1.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d8365-2871-41d2-9624-805c79ee66a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# amount of fraud classes 492 rows.\n",
    "fraud_df = df1.loc[df1['Class'] == 1]\n",
    "non_fraud_df = df1.loc[df1['Class'] == 0][:492]\n",
    "\n",
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc08e4f-940a-4266-bf63-d343490d5840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot Distribution of the Classes\n",
    "print('Distribution of the Classes in the subsample dataset')\n",
    "print(new_df['Class'].value_counts()/len(new_df))\n",
    "\n",
    "sns.countplot(x ='Class', data=new_df)\n",
    "plt.title('Equally Distributed Classes', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e5320-1eeb-47cf-a298-3d8d2fb1c3a0",
   "metadata": {},
   "source": [
    "## 1.3 VISUALIZATION WITH BALANCED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0544b258-f4e7-43f2-9939-0ab811ab16c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Histogram distribution\n",
    "for i in new_df.loc[:, ~new_df.columns.isin(['Time','Class','IsFraud','time_udf'])]:\n",
    "    plt.figure(figsize = [10,5]);\n",
    "    plt.subplot(1,2,1);\n",
    "    sns.histplot(new_df [i]);\n",
    "    plt.title('Distribution of {} values'.format(i) , size = 14);\n",
    "    plt.xlabel(i , size = 12);\n",
    "    plt.ylabel(\"count\", size = 12);\n",
    "    \n",
    "    plt.subplot(1,2,2);\n",
    "    sns.boxplot(data = new_df, x = i);\n",
    "    plt.title('{} boxplot'.format(i) , size = 14);\n",
    "    plt.xlabel(i , size = 12);\n",
    "    plt.ylabel(\"count\", size = 12);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7880a7-75ba-49c7-8d29-f03f20ee9e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Normality checking\n",
    "def is_normal(x, treshhold = 0.05):\n",
    "    k2,p = stats.normaltest(x)\n",
    "    print(p)\n",
    "    print(p > treshhold)\n",
    "    print('\\n')\n",
    "    return p > treshhold\n",
    "\n",
    "for name in list(new_df.loc[:, ~new_df.columns.isin(['Time','Class','IsFraud','time_udf'])]):\n",
    "    is_normal(np.array(new_df[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3862f6-d3a3-42c1-ab39-1e697658ff99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check skeness\n",
    "new_df.loc[:, ~new_df.columns.isin(['Time','Class','IsFraud','time_udf'])].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d9300-b2df-4c80-ad8f-0391f863f357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization of the relation between each variable and Class\n",
    "x = 0\n",
    "plt.figure(figsize = [18,20]);\n",
    "for i in new_df.loc[:, ~new_df.columns.isin(['Time','Class','IsFraud','time_udf'])] :\n",
    "    plt.subplot(8,4,x+1)\n",
    "    sns.boxplot(data = new_df, x = 'Class', y = i)\n",
    "    plt.title(\"barplot visualization between Class and {}\".format(i), size = 12);\n",
    "    x = x +1\n",
    "    plt.subplots_adjust(left=0.1, \n",
    "                    bottom=0.1,  \n",
    "                    right=0.9,  \n",
    "                    top=0.9,  \n",
    "                    wspace=0.3,  \n",
    "                    hspace=0.5) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329015b8-8ed2-4bc5-9cc8-d23c4be4d547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make sure we use the subsample in our correlation\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
    "\n",
    "df1= df.toPandas()\n",
    "df1= df1.sample(frac=1)\n",
    "df2 = df1.loc[:, ~df1.columns.isin(['Time','IsFraud','time_udf'])]\n",
    "new_df1= new_df.loc[:, ~new_df.columns.isin(['Time','IsFraud','time_udf'])]\n",
    "\n",
    "# Entire DataFrame\n",
    "corr = df2.corr()\n",
    "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\n",
    "ax1.set_title(\"Imbalanced Correlation Matrix\", fontsize=14)\n",
    "\n",
    "\n",
    "sub_sample_corr = new_df1.corr()\n",
    "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\n",
    "ax2.set_title('SubSample Correlation Matrix ', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fd6ba-304d-46af-af34-47519d3accb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Sorted Correlation values with Class:')\n",
    "print(new_df1[new_df1.columns[1:]].corr()['Class'][:-1].sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba295c5-d958-4210-81d6-dda625ba855d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make sure we use the subsample in our correlation\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
    "\n",
    "sub_sample_corr = new_df1.corr()\n",
    "\n",
    "sns.heatmap(sub_sample_corr > 0.7, cbar=False, annot_kws={'size':20}, ax=ax1)\n",
    "ax1.set_title('SubSample Correlation Matrix: Correlation > 0.7', fontsize=14)\n",
    "\n",
    "sns.heatmap(sub_sample_corr <-0.7,  cbar=False, annot_kws={'size':20}, ax=ax2)\n",
    "ax2.set_title('SubSample Correlation Matrix: Correlation < -0.7', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb65aeb-68ad-4e42-a7b7-c3c58ddc102a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from collections import OrderedDict\n",
    "corrs = OrderedDict([(col, pearsonr(new_df1[col], new_df1['Class'] == 1)) for col in new_df1.loc[:, ~new_df1.columns.isin(['Time','Class'])]])\n",
    "corrs = pd.DataFrame(index = corrs.keys(), data={\n",
    "        'corr_coef': [corr[0] for corr in corrs.values()],\n",
    "        'p_value': [corr[1] for corr in corrs.values()],\n",
    "    })\n",
    "\n",
    "corrs.applymap(lambda xx : abs(xx)).sort_values(by='corr_coef', ascending=False).rename(columns={\n",
    "        'corr_coef': 'absolute correlation coefficient'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d04464-f6b9-4cf0-930e-05e18f298674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Bivariate Analysis\n",
    "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
    "\n",
    "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df1,  ax=axes[0])\n",
    "axes[0].set_title('V14 vs Class Negative Correlation')\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df1,  ax=axes[1])\n",
    "axes[1].set_title('V12 vs Class Negative Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df1, ax=axes[2])\n",
    "axes[2].set_title('V10 vs Class Negative Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V16\", data=new_df1, ax=axes[3])\n",
    "axes[3].set_title('V16 vs Class Negative Correlation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec029a7b-02eb-408e-aa87-d9e24c66ac21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Bivariate Analysis\n",
    "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
    "\n",
    "# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\n",
    "sns.boxplot(x=\"Class\", y=\"V4\", data=new_df1,  ax=axes[0])\n",
    "axes[0].set_title('V4 vs Class Positive Correlation')\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V11\", data=new_df1,  ax=axes[1])\n",
    "axes[1].set_title('V11 vs Class Positive Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V2\", data=new_df1, ax=axes[2])\n",
    "axes[2].set_title('V2 vs Class Positive Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V19\", data=new_df1, ax=axes[3])\n",
    "axes[3].set_title('V19 vs Class Positive Correlation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75380cd-86dd-4bc5-a388-d437ff3a97cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Histogram distribution if is Fraud\n",
    "new_df1.loc[:, ~new_df1.columns.isin(['Time','Class'])].loc[new_df['Class'] == 1].hist(bins=30, figsize=(10, 10))\n",
    "plt.subplots_adjust(left=0.1, \n",
    "                    bottom=0.1,  \n",
    "                    right=0.9,  \n",
    "                    top=0.9,  \n",
    "                    wspace=0.4,  \n",
    "                    hspace=0.8) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40d12a-19e9-48e7-bd54-2abeda2cd22b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n",
    "\n",
    "v14_fraud_dist = new_df1['V14'].loc[new_df1['Class'] == 1].values\n",
    "sns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\n",
    "ax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "v12_fraud_dist = new_df1['V12'].loc[new_df1['Class'] == 1].values\n",
    "sns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\n",
    "ax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "\n",
    "v10_fraud_dist = new_df1['V10'].loc[new_df1['Class'] == 1].values\n",
    "sns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\n",
    "ax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e02258-2b1e-4dcb-9881-2157310c8fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in new_df1.columns:\n",
    "    plt.figure();\n",
    "    plt.tight_layout()\n",
    "    sns.set(rc={\"figure.figsize\":(8, 5)})\n",
    "    f, (ax_hist) = plt.subplots(1, sharex=True)\n",
    "    plt.gca().set(xlabel= i,ylabel='Density')\n",
    "    #sns.histplot(new_df[i], ax=ax_hist ,  bins = 20, kde=True)\n",
    "    sns.distplot(new_df1[i], ax=ax_hist, fit=norm, color='#FB8861')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649163e5-19f1-4e0d-8a0f-f6f12fb07c24",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86470f-9cee-47e1-9790-d1df197f126a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z = np.abs(stats.zscore(new_df1.loc[:, ~new_df1.columns.isin(['Time','Class'])]))\n",
    "threshold = 3\n",
    "df1_new = new_df1[(z < 3).all(axis=1)]\n",
    "\n",
    "df1_new.describe().T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0b60f-bee6-4801-a6a5-2e05efbf04ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df1.loc[:, ~new_df1.columns.isin(['Amount','Time','Class', 'IsFraud','time_udf'])].boxplot( figsize=(12,8), vert=False)\n",
    "plt.title(\"With outliers\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c9b74-e14a-4a9d-9416-921266d88456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1_new.loc[:, ~df1_new.columns.isin(['Amount','Time','Class'])].boxplot( figsize=(12,8), vert=False)\n",
    "plt.title(\"Without outliers\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ccade-e81a-4e48-bfd3-3231ad96919a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# % of data removed :\n",
    "print(\"percentage of records removed is :\",(1 - (df1_new.shape[0] / new_df1.shape[0]))*100,\", it is an accepted % \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976204d-2436-4191-9571-bd7e2a217d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Distribution of the Classes\n",
    "print('Distribution of the Classes in the dataset without outliers')\n",
    "print(df1_new['Class'].value_counts()/len(df1_new))\n",
    "\n",
    "sns.countplot(x ='Class', data=df1_new)\n",
    "plt.title('Balanced Classes without outliers', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112fdb5-c7e7-4dae-b7c2-a2ce11b48413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization of the relation between each variable and Class in a balanced dataset without outliers\n",
    "x = 0\n",
    "plt.figure(figsize = [18,20]);\n",
    "for i in df1_new.loc[:, ~df1_new.columns.isin(['Time','Class'])] :\n",
    "    plt.subplot(8,4,x+1)\n",
    "    sns.boxplot(data = df1_new, x = 'Class', y = i)\n",
    "    plt.title(\"barplot visualization between Class and {}\".format(i), size = 12);\n",
    "    x = x +1\n",
    "    plt.subplots_adjust(left=0.1, \n",
    "                    bottom=0.1,  \n",
    "                    right=0.9,  \n",
    "                    top=0.9,  \n",
    "                    wspace=0.3,  \n",
    "                    hspace=0.5) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48c03d-7a95-49a2-b85d-90021b3c2758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n",
    "colors = ['#B3F9C5', '#f9c5b3']\n",
    "# Boxplots with outliers removed\n",
    "# Feature V14\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=df1_new,ax=ax1, palette=colors)\n",
    "ax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\n",
    "ax1.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n",
    "             arrowprops=dict(facecolor='black'),\n",
    "             fontsize=14)\n",
    "# Feature 12\n",
    "sns.boxplot(x=\"Class\", y=\"V12\", data=df1_new, ax=ax2, palette=colors)\n",
    "ax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\n",
    "ax2.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.3), xytext=(0, -12),\n",
    "             arrowprops=dict(facecolor='black'),\n",
    "             fontsize=14)\n",
    "# Feature V10\n",
    "sns.boxplot(x=\"Class\", y=\"V10\", data=df1_new, ax=ax3, palette=colors)\n",
    "ax3.set_title(\"V10 Feature \\n Reduction of outliers\", fontsize=14)\n",
    "ax3.annotate('Fewer extreme \\n outliers', xy=(0.95, -16.5), xytext=(0, -12),\n",
    "             arrowprops=dict(facecolor='black'),\n",
    "             fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc24b1bd-9f3f-4bc0-850d-b0c650b18b50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Machine Learning - É PRECISO REVER TUDO DAQUI PARA BAIXO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f3369-d2c2-4b7c-bd48-b611dc9cdfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing required Spark ML lib methods\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9bbbee-ff5e-4032-985a-3a55d79f64b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfsp = spark.createDataFrame(new_df.loc[:, ~new_df.columns.isin(['IsFraud','time_udf'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a189652b-eeb6-424a-8cf2-9442dbb57f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting String data type of column to double\n",
    "\n",
    "for column in dfsp.columns:\n",
    "    data = dfsp.withColumn(column,dfsp[column].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102008d2-89b0-45e3-95ee-431d4f2ea31d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Adding index to keep track of the rows even after shuffling\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "win = Window().orderBy('Time')\n",
    "dfsp = dfsp.withColumn(\"idx\", row_number().over(win))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3155d09f-3cba-4979-8381-99f686f12e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfsp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d636a-c484-4609-a21e-6311389a981d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_columns = [col for col in dfsp.columns if col.startswith(\"V\")]\n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8869648-ca57-4f73-b69c-e75d125e76b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = VectorAssembler(inputCols = feature_columns, outputCol=\"features\")\n",
    "vectorizer.transform(df).select(\"features\", \"Class\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32663b-29e9-4598-9838-4f1a7fd8747f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "est = RandomForestClassifier()\n",
    "est.setMaxDepth(5)\n",
    "est.setLabelCol(\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b48b5-6020-4cfa-b360-6e9244fc1d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(est.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b78819-c38d-429d-ad8a-c89b6317bce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing required Spark ML lib methods\n",
    "\n",
    "from pyspark.ml.pipeline import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ff0b3-d4b7-4116-9eb1-15d3bd23302a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting the feature columns to dense vector (required by spark) and creating label and index columns\n",
    "\n",
    "training_df = dfsp.rdd.map(lambda x: (DenseVector(x[0:29]),x[30],x[31]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa86c11-547a-40f3-a380-97b3d2abe483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_df = spark.createDataFrame(training_df,[\"features\",\"label\",\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de61c5-7a32-4e97-8f13-5b3b746dfa8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_df = training_df.select(\"index\",\"features\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f27c95-caf4-4437-b20d-fe981ae484e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting data into training and testing data\n",
    "\n",
    "train_data, test_data = training_df.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0a09a-38ca-42fd-8f5d-86d861948c84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8509b-3313-498e-bc1c-b7fc1cdc1b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf35f82-8937-4fd2-9441-0ee582b2e58c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train, df_test = df.randomSplit(weights=[0.7, 0.3], seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd65510d-84ea-4cfa-8b2f-1788e514fd7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.setStages([vectorizer, est])\n",
    "model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597ec01-dc4b-4630-9990-8e84772832f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df_test_pred = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91833a6-9d9a-48bb-9607-ac2a84137923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7da561-40c0-4c71-9bd5-6dc65842b975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d5130b-2ac7-474d-9b2e-31afa11c73f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator.evaluate(model.transform(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365e221-bbfb-4332-a432-305b0035cea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928f577-9472-493f-9811-6aaee0349023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_accuracy = (df_test_pred\n",
    "                 .select(\"Class\", \"prediction\")\n",
    "                 .withColumn(\"isEqual\", expr(\"Class == prediction\"))\n",
    "                 .select(avg(expr(\"cast(isEqual as float)\")))\n",
    "                 .first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae3553-a3ad-4ea6-a108-6a94e8133f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d747ca6-2c51-4800-b780-c1c781fef62d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "treeEstimator = DecisionTreeClassifier()\n",
    "treeEstimator.setImpurity(\"entropy\")\n",
    "treeEstimator.setLabelCol(\"Class\")\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.setStages([vectorizer, treeEstimator])\n",
    "model = pipeline.fit(df_train)\n",
    "evaluator.evaluate(model.transform(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700521e-021b-4605-86a7-670bab926deb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_evaluator = MulticlassClassificationEvaluator()\n",
    "accuracy_evaluator.setLabelCol(\"Class\")\n",
    "accuracy_evaluator.setMetricName(\"accuracy\")\n",
    "accuracy_evaluator.evaluate(model.transform(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbfc7e7-aeb0-46bb-996d-d34a16a87ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_evaluator = MulticlassClassificationEvaluator()\n",
    "f1_evaluator.setLabelCol(\"Class\")\n",
    "f1_evaluator.setMetricName(\"f1\")\n",
    "f1_evaluator.evaluate(model.transform(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1efa8-3d38-4ad5-8a7d-9d80df9d7a26",
   "metadata": {},
   "source": [
    "## Gradient Boosting Trees Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70966d19-8318-4163-b3a4-1ec5e24027eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating Gradient Boosting Trees Classifier Model to fit and predict data\n",
    "\n",
    "gbt = GBTClassifier(featuresCol=\"features\", maxIter=100,maxDepth=8)\n",
    "model = gbt.fit(train_data)\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f741d8-5047-4c2f-912d-7a496b4381b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking the count of records classified into each classes\n",
    "\n",
    "predictions.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3bcb8-9f8b-4824-a6b6-2fbb59ec9f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating accuracy of model\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63da320c-e980-4edb-a3e2-bb2bc00f524d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating percentage of fraud records predicted correctly\n",
    "\n",
    "predictions = predictions.withColumn(\"fraudPrediction\",when((predictions.label==1)&(predictions.prediction==1),1).otherwise(0))\n",
    "predictions.groupBy(\"fraudPrediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cdd08c-aa13-4f53-9bd3-6dd77b8b9389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6377ad-2825-4866-8abf-6e4d9c298b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accurateFraud = predictions.groupBy(\"fraudPrediction\").count().where(predictions.fraudPrediction==1).head()[1]\n",
    "totalFraud = predictions.groupBy(\"label\").count().where(predictions.label==1).head()[1]\n",
    "FraudPredictionAccuracy = (accurateFraud/totalFraud)*100\n",
    "print(\"Fraud Prediction Accuracy: \",FraudPredictionAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c6da2-071f-4b13-8046-59083585f618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating Confusion matrix\n",
    "\n",
    "tp = predictions[(predictions.label == 1) & (predictions.prediction == 1)].count()\n",
    "tn = predictions[(predictions.label == 0) & (predictions.prediction == 0)].count()\n",
    "fp = predictions[(predictions.label == 0) & (predictions.prediction == 1)].count()\n",
    "fn = predictions[(predictions.label == 1) & (predictions.prediction == 0)].count()\n",
    "print(\"True Positive: \",tp,\"\\nTrue Negative: \",tn,\"\\nFalse Positive: \",fp,\"\\nFalse Negative: \",fn)\n",
    "print(\"Recall: \",tp/(tp+fn))\n",
    "print(\"Precision: \", tp/(tp+fp))\n",
    "print(\"F1 Score\",  (2 * (tp/(tp+fp)) * (tp/(tp+fn)) /((tp/(tp+fp)) + (tp/(tp+fn)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681aefc-38d5-4a23-a50a-0c4b3b94b96f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
