{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Credit Card Fraud\n",
    "\n",
    "Fraud activities are considered uncommon or outliers transactions, which is probably one of the main characteristics regarding Fraud. As the authors of the book Fraud Analytics using Descriptive, Predictive, and Social Network Techniques: A Guide to Data Science for Fraud Detection pointed out:\n",
    "\n",
    "“This makes it difficult [because of the outlier characteristic] to both detect fraud, since the fraudulent cases are covered by the nonfraudulent ones, as well as to learn from historical cases to build a powerful fraud-detection system since only few examples are available”\n",
    "\n",
    "Hence, it is imperative to overcome this issue considering the unbalanced nature of the data.\n",
    "\n",
    "In this project, we will use the credit card data available in [https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud). Moreover, we will use PySpark, an Interface for Apache Spark in Python, since it is an excellent tool dealing with Big Data.\n",
    "\n",
    "### Feature Technicalities:\n",
    "\n",
    "- PCA Transformation: The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) (Except for time and amount).\n",
    "- Scaling: In order to implement a PCA transformation features need to be previously scaled. (In this case, all the V features have been scaled)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA - Exploratory Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import PySpark libraries\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LinearSVC, NaiveBayes\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Configure Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Credit Card Fraud\") \\\n",
    "    .config(\"spark.master\", \"local\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T08:56:00.916538Z",
     "end_time": "2023-04-30T08:56:00.973299Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load file\n",
    "df = spark.read.csv('F:/VULCANO/07_MECD/2_Semestre/EGD/trabalho_C_Card/creditcard-egd-main_RS/datasets/creditcard.csv', header=True, inferSchema=True, sep=\",\")\n",
    "# Print Schema\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T23:52:05.700988Z",
     "end_time": "2023-04-29T23:52:15.288511Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 31\n",
      "Number of Records: 284807\n"
     ]
    }
   ],
   "source": [
    "# Spark DataFrame Shape\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "print(f\"Number of Records: {df.count()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T23:54:36.541327Z",
     "end_time": "2023-04-29T23:54:37.915127Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset contains **284,807 records** and consists of **31 columns**. Out of these columns, **28 columns (V1 to V28)** are results of the **Principal Component Analysis (PCA)** technique. These columns are likely to be more abstract in nature since they are generated from linear combinations of the original features. Therefore, it may be difficult to interpret the individual contributions of these features towards the prediction task.\n",
    "\n",
    "On the other hand, the remaining three columns are:\n",
    "\n",
    "- **Time**: This column indicates the time elapsed in seconds between each transaction and the first transaction in the dataset.\n",
    "- **Amount**: This column indicates the transaction amount.\n",
    "- **Class**: This column indicates the fraud status of the transaction. The value 1 indicates fraud, and 0 indicates a legitimate transaction.\n",
    "\n",
    "It is important to note that since the **PCA** technique is used, the original feature names and descriptions are not available. Therefore, feature engineering might be necessary to extract meaningful features from the given dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n5  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427 -0.232794   \n6  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055  0.750137   \n7 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709 -0.415267   \n8  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592  0.373205   \n9  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794 -0.385050 -0.069733   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n5  0.105915  0.253844  0.081080    3.67      0  \n6 -0.257237  0.034507  0.005168    4.99      0  \n7 -0.051634 -1.206921 -1.085339   40.80      0  \n8 -0.384157  0.011747  0.142404   93.20      0  \n9  0.094199  0.246219  0.083076    3.68      0  \n\n[10 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2.0</td>\n      <td>-0.425966</td>\n      <td>0.960523</td>\n      <td>1.141109</td>\n      <td>-0.168252</td>\n      <td>0.420987</td>\n      <td>-0.029728</td>\n      <td>0.476201</td>\n      <td>0.260314</td>\n      <td>-0.568671</td>\n      <td>...</td>\n      <td>-0.208254</td>\n      <td>-0.559825</td>\n      <td>-0.026398</td>\n      <td>-0.371427</td>\n      <td>-0.232794</td>\n      <td>0.105915</td>\n      <td>0.253844</td>\n      <td>0.081080</td>\n      <td>3.67</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4.0</td>\n      <td>1.229658</td>\n      <td>0.141004</td>\n      <td>0.045371</td>\n      <td>1.202613</td>\n      <td>0.191881</td>\n      <td>0.272708</td>\n      <td>-0.005159</td>\n      <td>0.081213</td>\n      <td>0.464960</td>\n      <td>...</td>\n      <td>-0.167716</td>\n      <td>-0.270710</td>\n      <td>-0.154104</td>\n      <td>-0.780055</td>\n      <td>0.750137</td>\n      <td>-0.257237</td>\n      <td>0.034507</td>\n      <td>0.005168</td>\n      <td>4.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7.0</td>\n      <td>-0.644269</td>\n      <td>1.417964</td>\n      <td>1.074380</td>\n      <td>-0.492199</td>\n      <td>0.948934</td>\n      <td>0.428118</td>\n      <td>1.120631</td>\n      <td>-3.807864</td>\n      <td>0.615375</td>\n      <td>...</td>\n      <td>1.943465</td>\n      <td>-1.015455</td>\n      <td>0.057504</td>\n      <td>-0.649709</td>\n      <td>-0.415267</td>\n      <td>-0.051634</td>\n      <td>-1.206921</td>\n      <td>-1.085339</td>\n      <td>40.80</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7.0</td>\n      <td>-0.894286</td>\n      <td>0.286157</td>\n      <td>-0.113192</td>\n      <td>-0.271526</td>\n      <td>2.669599</td>\n      <td>3.721818</td>\n      <td>0.370145</td>\n      <td>0.851084</td>\n      <td>-0.392048</td>\n      <td>...</td>\n      <td>-0.073425</td>\n      <td>-0.268092</td>\n      <td>-0.204233</td>\n      <td>1.011592</td>\n      <td>0.373205</td>\n      <td>-0.384157</td>\n      <td>0.011747</td>\n      <td>0.142404</td>\n      <td>93.20</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9.0</td>\n      <td>-0.338262</td>\n      <td>1.119593</td>\n      <td>1.044367</td>\n      <td>-0.222187</td>\n      <td>0.499361</td>\n      <td>-0.246761</td>\n      <td>0.651583</td>\n      <td>0.069539</td>\n      <td>-0.736727</td>\n      <td>...</td>\n      <td>-0.246914</td>\n      <td>-0.633753</td>\n      <td>-0.120794</td>\n      <td>-0.385050</td>\n      <td>-0.069733</td>\n      <td>0.094199</td>\n      <td>0.246219</td>\n      <td>0.083076</td>\n      <td>3.68</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting the first 10 records\n",
    "df.limit(10).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T23:56:44.752675Z",
     "end_time": "2023-04-29T23:56:45.214888Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "           count          mean           std         min           25%  \\\nTime    284807.0  9.481386e+04  47488.145955    0.000000  54201.500000   \nV1      284807.0  3.918649e-15      1.958696  -56.407510     -0.920373   \nV2      284807.0  5.682686e-16      1.651309  -72.715728     -0.598550   \nV3      284807.0 -8.761736e-15      1.516255  -48.325589     -0.890365   \nV4      284807.0  2.811118e-15      1.415869   -5.683171     -0.848640   \nV5      284807.0 -1.552103e-15      1.380247 -113.743307     -0.691597   \nV6      284807.0  2.040130e-15      1.332271  -26.160506     -0.768296   \nV7      284807.0 -1.698953e-15      1.237094  -43.557242     -0.554076   \nV8      284807.0 -1.958151e-16      1.194353  -73.216718     -0.208630   \nV9      284807.0 -3.147640e-15      1.098632  -13.434066     -0.643098   \nV10     284807.0  1.772925e-15      1.088850  -24.588262     -0.535426   \nV11     284807.0  9.289524e-16      1.020713   -4.797473     -0.762494   \nV12     284807.0 -1.803266e-15      0.999201  -18.683715     -0.405571   \nV13     284807.0  1.674888e-15      0.995274   -5.791881     -0.648539   \nV14     284807.0  1.475621e-15      0.958596  -19.214325     -0.425574   \nV15     284807.0  3.501098e-15      0.915316   -4.498945     -0.582884   \nV16     284807.0  1.392438e-15      0.876253  -14.129855     -0.468037   \nV17     284807.0 -7.466538e-16      0.849337  -25.162799     -0.483748   \nV18     284807.0  4.258738e-16      0.838176   -9.498746     -0.498850   \nV19     284807.0  9.020169e-16      0.814041   -7.213527     -0.456299   \nV20     284807.0  5.126845e-16      0.770925  -54.497720     -0.211721   \nV21     284807.0  1.471982e-16      0.734524  -34.830382     -0.228395   \nV22     284807.0  8.042109e-16      0.725702  -10.933144     -0.542350   \nV23     284807.0  5.282450e-16      0.624460  -44.807735     -0.161846   \nV24     284807.0  4.458267e-15      0.605647   -2.836627     -0.354586   \nV25     284807.0  1.426896e-15      0.521278  -10.295397     -0.317145   \nV26     284807.0  1.701640e-15      0.482227   -2.604551     -0.326984   \nV27     284807.0 -3.671606e-16      0.403632  -22.565679     -0.070840   \nV28     284807.0 -1.218152e-16      0.330083  -15.430084     -0.052960   \nAmount  284807.0  8.834962e+01    250.120109    0.000000      5.600000   \nClass   284807.0  1.727486e-03      0.041527    0.000000      0.000000   \n\n                 50%            75%            max  \nTime    84692.000000  139320.500000  172792.000000  \nV1          0.018109       1.315642       2.454930  \nV2          0.065486       0.803724      22.057729  \nV3          0.179846       1.027196       9.382558  \nV4         -0.019847       0.743341      16.875344  \nV5         -0.054336       0.611926      34.801666  \nV6         -0.274187       0.398565      73.301626  \nV7          0.040103       0.570436     120.589494  \nV8          0.022358       0.327346      20.007208  \nV9         -0.051429       0.597139      15.594995  \nV10        -0.092917       0.453923      23.745136  \nV11        -0.032757       0.739593      12.018913  \nV12         0.140033       0.618238       7.848392  \nV13        -0.013568       0.662505       7.126883  \nV14         0.050601       0.493150      10.526766  \nV15         0.048072       0.648821       8.877742  \nV16         0.066413       0.523296      17.315112  \nV17        -0.065676       0.399675       9.253526  \nV18        -0.003636       0.500807       5.041069  \nV19         0.003735       0.458949       5.591971  \nV20        -0.062481       0.133041      39.420904  \nV21        -0.029450       0.186377      27.202839  \nV22         0.006782       0.528554      10.503090  \nV23        -0.011193       0.147642      22.528412  \nV24         0.040976       0.439527       4.584549  \nV25         0.016594       0.350716       7.519589  \nV26        -0.052139       0.240952       3.517346  \nV27         0.001342       0.091045      31.612198  \nV28         0.011244       0.078280      33.847808  \nAmount     22.000000      77.165000   25691.160000  \nClass       0.000000       0.000000       1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Time</th>\n      <td>284807.0</td>\n      <td>9.481386e+04</td>\n      <td>47488.145955</td>\n      <td>0.000000</td>\n      <td>54201.500000</td>\n      <td>84692.000000</td>\n      <td>139320.500000</td>\n      <td>172792.000000</td>\n    </tr>\n    <tr>\n      <th>V1</th>\n      <td>284807.0</td>\n      <td>3.918649e-15</td>\n      <td>1.958696</td>\n      <td>-56.407510</td>\n      <td>-0.920373</td>\n      <td>0.018109</td>\n      <td>1.315642</td>\n      <td>2.454930</td>\n    </tr>\n    <tr>\n      <th>V2</th>\n      <td>284807.0</td>\n      <td>5.682686e-16</td>\n      <td>1.651309</td>\n      <td>-72.715728</td>\n      <td>-0.598550</td>\n      <td>0.065486</td>\n      <td>0.803724</td>\n      <td>22.057729</td>\n    </tr>\n    <tr>\n      <th>V3</th>\n      <td>284807.0</td>\n      <td>-8.761736e-15</td>\n      <td>1.516255</td>\n      <td>-48.325589</td>\n      <td>-0.890365</td>\n      <td>0.179846</td>\n      <td>1.027196</td>\n      <td>9.382558</td>\n    </tr>\n    <tr>\n      <th>V4</th>\n      <td>284807.0</td>\n      <td>2.811118e-15</td>\n      <td>1.415869</td>\n      <td>-5.683171</td>\n      <td>-0.848640</td>\n      <td>-0.019847</td>\n      <td>0.743341</td>\n      <td>16.875344</td>\n    </tr>\n    <tr>\n      <th>V5</th>\n      <td>284807.0</td>\n      <td>-1.552103e-15</td>\n      <td>1.380247</td>\n      <td>-113.743307</td>\n      <td>-0.691597</td>\n      <td>-0.054336</td>\n      <td>0.611926</td>\n      <td>34.801666</td>\n    </tr>\n    <tr>\n      <th>V6</th>\n      <td>284807.0</td>\n      <td>2.040130e-15</td>\n      <td>1.332271</td>\n      <td>-26.160506</td>\n      <td>-0.768296</td>\n      <td>-0.274187</td>\n      <td>0.398565</td>\n      <td>73.301626</td>\n    </tr>\n    <tr>\n      <th>V7</th>\n      <td>284807.0</td>\n      <td>-1.698953e-15</td>\n      <td>1.237094</td>\n      <td>-43.557242</td>\n      <td>-0.554076</td>\n      <td>0.040103</td>\n      <td>0.570436</td>\n      <td>120.589494</td>\n    </tr>\n    <tr>\n      <th>V8</th>\n      <td>284807.0</td>\n      <td>-1.958151e-16</td>\n      <td>1.194353</td>\n      <td>-73.216718</td>\n      <td>-0.208630</td>\n      <td>0.022358</td>\n      <td>0.327346</td>\n      <td>20.007208</td>\n    </tr>\n    <tr>\n      <th>V9</th>\n      <td>284807.0</td>\n      <td>-3.147640e-15</td>\n      <td>1.098632</td>\n      <td>-13.434066</td>\n      <td>-0.643098</td>\n      <td>-0.051429</td>\n      <td>0.597139</td>\n      <td>15.594995</td>\n    </tr>\n    <tr>\n      <th>V10</th>\n      <td>284807.0</td>\n      <td>1.772925e-15</td>\n      <td>1.088850</td>\n      <td>-24.588262</td>\n      <td>-0.535426</td>\n      <td>-0.092917</td>\n      <td>0.453923</td>\n      <td>23.745136</td>\n    </tr>\n    <tr>\n      <th>V11</th>\n      <td>284807.0</td>\n      <td>9.289524e-16</td>\n      <td>1.020713</td>\n      <td>-4.797473</td>\n      <td>-0.762494</td>\n      <td>-0.032757</td>\n      <td>0.739593</td>\n      <td>12.018913</td>\n    </tr>\n    <tr>\n      <th>V12</th>\n      <td>284807.0</td>\n      <td>-1.803266e-15</td>\n      <td>0.999201</td>\n      <td>-18.683715</td>\n      <td>-0.405571</td>\n      <td>0.140033</td>\n      <td>0.618238</td>\n      <td>7.848392</td>\n    </tr>\n    <tr>\n      <th>V13</th>\n      <td>284807.0</td>\n      <td>1.674888e-15</td>\n      <td>0.995274</td>\n      <td>-5.791881</td>\n      <td>-0.648539</td>\n      <td>-0.013568</td>\n      <td>0.662505</td>\n      <td>7.126883</td>\n    </tr>\n    <tr>\n      <th>V14</th>\n      <td>284807.0</td>\n      <td>1.475621e-15</td>\n      <td>0.958596</td>\n      <td>-19.214325</td>\n      <td>-0.425574</td>\n      <td>0.050601</td>\n      <td>0.493150</td>\n      <td>10.526766</td>\n    </tr>\n    <tr>\n      <th>V15</th>\n      <td>284807.0</td>\n      <td>3.501098e-15</td>\n      <td>0.915316</td>\n      <td>-4.498945</td>\n      <td>-0.582884</td>\n      <td>0.048072</td>\n      <td>0.648821</td>\n      <td>8.877742</td>\n    </tr>\n    <tr>\n      <th>V16</th>\n      <td>284807.0</td>\n      <td>1.392438e-15</td>\n      <td>0.876253</td>\n      <td>-14.129855</td>\n      <td>-0.468037</td>\n      <td>0.066413</td>\n      <td>0.523296</td>\n      <td>17.315112</td>\n    </tr>\n    <tr>\n      <th>V17</th>\n      <td>284807.0</td>\n      <td>-7.466538e-16</td>\n      <td>0.849337</td>\n      <td>-25.162799</td>\n      <td>-0.483748</td>\n      <td>-0.065676</td>\n      <td>0.399675</td>\n      <td>9.253526</td>\n    </tr>\n    <tr>\n      <th>V18</th>\n      <td>284807.0</td>\n      <td>4.258738e-16</td>\n      <td>0.838176</td>\n      <td>-9.498746</td>\n      <td>-0.498850</td>\n      <td>-0.003636</td>\n      <td>0.500807</td>\n      <td>5.041069</td>\n    </tr>\n    <tr>\n      <th>V19</th>\n      <td>284807.0</td>\n      <td>9.020169e-16</td>\n      <td>0.814041</td>\n      <td>-7.213527</td>\n      <td>-0.456299</td>\n      <td>0.003735</td>\n      <td>0.458949</td>\n      <td>5.591971</td>\n    </tr>\n    <tr>\n      <th>V20</th>\n      <td>284807.0</td>\n      <td>5.126845e-16</td>\n      <td>0.770925</td>\n      <td>-54.497720</td>\n      <td>-0.211721</td>\n      <td>-0.062481</td>\n      <td>0.133041</td>\n      <td>39.420904</td>\n    </tr>\n    <tr>\n      <th>V21</th>\n      <td>284807.0</td>\n      <td>1.471982e-16</td>\n      <td>0.734524</td>\n      <td>-34.830382</td>\n      <td>-0.228395</td>\n      <td>-0.029450</td>\n      <td>0.186377</td>\n      <td>27.202839</td>\n    </tr>\n    <tr>\n      <th>V22</th>\n      <td>284807.0</td>\n      <td>8.042109e-16</td>\n      <td>0.725702</td>\n      <td>-10.933144</td>\n      <td>-0.542350</td>\n      <td>0.006782</td>\n      <td>0.528554</td>\n      <td>10.503090</td>\n    </tr>\n    <tr>\n      <th>V23</th>\n      <td>284807.0</td>\n      <td>5.282450e-16</td>\n      <td>0.624460</td>\n      <td>-44.807735</td>\n      <td>-0.161846</td>\n      <td>-0.011193</td>\n      <td>0.147642</td>\n      <td>22.528412</td>\n    </tr>\n    <tr>\n      <th>V24</th>\n      <td>284807.0</td>\n      <td>4.458267e-15</td>\n      <td>0.605647</td>\n      <td>-2.836627</td>\n      <td>-0.354586</td>\n      <td>0.040976</td>\n      <td>0.439527</td>\n      <td>4.584549</td>\n    </tr>\n    <tr>\n      <th>V25</th>\n      <td>284807.0</td>\n      <td>1.426896e-15</td>\n      <td>0.521278</td>\n      <td>-10.295397</td>\n      <td>-0.317145</td>\n      <td>0.016594</td>\n      <td>0.350716</td>\n      <td>7.519589</td>\n    </tr>\n    <tr>\n      <th>V26</th>\n      <td>284807.0</td>\n      <td>1.701640e-15</td>\n      <td>0.482227</td>\n      <td>-2.604551</td>\n      <td>-0.326984</td>\n      <td>-0.052139</td>\n      <td>0.240952</td>\n      <td>3.517346</td>\n    </tr>\n    <tr>\n      <th>V27</th>\n      <td>284807.0</td>\n      <td>-3.671606e-16</td>\n      <td>0.403632</td>\n      <td>-22.565679</td>\n      <td>-0.070840</td>\n      <td>0.001342</td>\n      <td>0.091045</td>\n      <td>31.612198</td>\n    </tr>\n    <tr>\n      <th>V28</th>\n      <td>284807.0</td>\n      <td>-1.218152e-16</td>\n      <td>0.330083</td>\n      <td>-15.430084</td>\n      <td>-0.052960</td>\n      <td>0.011244</td>\n      <td>0.078280</td>\n      <td>33.847808</td>\n    </tr>\n    <tr>\n      <th>Amount</th>\n      <td>284807.0</td>\n      <td>8.834962e+01</td>\n      <td>250.120109</td>\n      <td>0.000000</td>\n      <td>5.600000</td>\n      <td>22.000000</td>\n      <td>77.165000</td>\n      <td>25691.160000</td>\n    </tr>\n    <tr>\n      <th>Class</th>\n      <td>284807.0</td>\n      <td>1.727486e-03</td>\n      <td>0.041527</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#statistics\n",
    "df.toPandas().describe().T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:01:51.570377Z",
     "end_time": "2023-04-30T00:01:58.666804Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset contains a total of **284807 records**. The **time column** in the dataset indicates the time elapsed in seconds between each transaction and the first transaction in the dataset. The maximum time recorded between transactions is **172,792 seconds**. This is equivalent to 2880 minutes or approximately **48 hours**. Since the timeframe of the dataset is only 2 days, it means that the last transaction was made two days after the first one.\n",
    "\n",
    "The amount column indicates the transaction amount. The biggest transaction recorded in the dataset is worth **$25,691.16**.\n",
    "\n",
    "\n",
    "## Checking Missing Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "|Time| V1| V2| V3| V4| V5| V6| V7| V8| V9|V10|V11|V12|V13|V14|V15|V16|V17|V18|V19|V20|V21|V22|V23|V24|V25|V26|V27|V28|Amount|Class|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|     0|    0|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check missing and null data\n",
    "df.select([f.count(f.when(f.isnan(c) | f.col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:06:48.080214Z",
     "end_time": "2023-04-30T00:06:52.356408Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upon checking the dataset, it was observed that there are no missing values present.\n",
    "Each column was inspected for null or NaN values, and it was found that the dataset is complete!\n",
    "\n",
    "\n",
    "## 1.2 Univariate Analysis\n",
    "\n",
    "### 1.2.1 Class\n",
    "\n",
    "#### Q1. HOW MANY ROWS IN THE DATASET REPRESENT CREDIT CARD FRAUD?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Class| count|\n",
      "+-----+------+\n",
      "|    1|   492|\n",
      "|    0|284315|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Number of Frauds and non-frauds\n",
    "classFreq = df.groupBy(\"Class\").count()\n",
    "classFreq.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:09:48.132209Z",
     "end_time": "2023-04-30T00:09:50.091811Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+\n",
      "|Class| count|percent|\n",
      "+-----+------+-------+\n",
      "|    1|   492|   0.17|\n",
      "|    0|284315|  99.83|\n",
      "+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total = classFreq.select(\"count\").agg({\"count\": \"sum\"}).collect().pop()['sum(count)']\n",
    "result = classFreq.withColumn('percent', f.format_number(classFreq['count']/total * 100, 2))\n",
    "result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:12:03.510675Z",
     "end_time": "2023-04-30T00:12:06.220339Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Out of the total of **284,807 records**, only **492** represent credit card fraud, which is approximately **0.17%** of the dataset. Therefore, we have an imbalanced dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a pandas dataframe with only the Class column\n",
    "fraud_df = df.select(\"Class\").toPandas()\n",
    "\n",
    "# Count the number of transactions for each class (fraudulent or not)\n",
    "fraud_counts = fraud_df['Class'].value_counts()\n",
    "\n",
    "# Calculate the percentage and absolute number of each class\n",
    "fraud_perc = fraud_counts / fraud_counts.sum() * 100\n",
    "fraud_abs = fraud_counts.values\n",
    "\n",
    "# Create a bar plot with logarithmic y-axis\n",
    "sns.barplot(x=fraud_counts.index, y=fraud_counts.values)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Add percentage and absolute value labels to the bars\n",
    "for i, v in enumerate(fraud_counts):\n",
    "    plt.text(i, v, f\"{fraud_perc[i]:.1f}% ({fraud_abs[i]:,})\", ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Set the y-axis labels and formatter\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: '{:.0f} K'.format(x/1000)))\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel('Transaction Class')\n",
    "plt.xticks([0, 1], ['Non-Fraud', 'Fraud'])\n",
    "plt.title('Distribution of fraudulent transactions')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:15:17.636462Z",
     "end_time": "2023-04-30T00:15:19.734681Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned earlier, the majority of transactions are legitimate. If we use this dataset as the foundation for our predictive models and analysis, we may encounter many errors, and our algorithms are likely to overfit since they will \"presume\" that most transactions are not fraudulent. However, we do not want our model to presume; we want our model to identify patterns that indicate fraud.\n",
    "\n",
    "\n",
    "\n",
    "### 1.2.2 Amount -- Should we remove this? At least for me, doesnt make much sense."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o351.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 24) (DESKTOP-9VU4VJO.home executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3997)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3994)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_13424\\1116563235.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'time_udf'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtime_udf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Time'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoPandas\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\pandas\\conversion.py\u001B[0m in \u001B[0;36mtoPandas\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    207\u001B[0m         \u001B[1;31m# Below is toPandas without Arrow optimization.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 208\u001B[1;33m         \u001B[0mpdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_records\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    209\u001B[0m         \u001B[0mcolumn_counter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCounter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\dataframe.py\u001B[0m in \u001B[0;36mcollect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1214\u001B[0m         \"\"\"\n\u001B[0;32m   1215\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1216\u001B[1;33m             \u001B[0msock_info\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcollectToPython\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1217\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mBatchedSerializer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mCPickleSerializer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1218\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1320\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1321\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1322\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1323\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1324\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    167\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    168\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 169\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    170\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    171\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    325\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 326\u001B[1;33m                 raise Py4JJavaError(\n\u001B[0m\u001B[0;32m    327\u001B[0m                     \u001B[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o351.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 24) (DESKTOP-9VU4VJO.home executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3997)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3994)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "# create a function to group time values\n",
    "@f.udf(returnType=t.StringType())\n",
    "def time_udf(time):\n",
    "    if time < 50000:\n",
    "        return \"Under 50K s\"\n",
    "    elif 50000 <= time <= 100000:\n",
    "        return \"Between 50K and 100K s\"\n",
    "    elif time > 100000:\n",
    "        return \"Over 100K s\"\n",
    "    else:\n",
    "        return \"NA\"\n",
    "\n",
    "# apply the function to the \"Time\" column\n",
    "df = df.withColumn('time_udf', time_udf('Time'))\n",
    "\n",
    "df.limit(10).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:24:40.089339Z",
     "end_time": "2023-04-30T00:24:42.937067Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o399.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 25) (DESKTOP-9VU4VJO.home executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 27 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_13424\\3495942735.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[0morderBy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdesc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Percent\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m \u001B[0mtime_group_table\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoPandas\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\pandas\\conversion.py\u001B[0m in \u001B[0;36mtoPandas\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    207\u001B[0m         \u001B[1;31m# Below is toPandas without Arrow optimization.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 208\u001B[1;33m         \u001B[0mpdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_records\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    209\u001B[0m         \u001B[0mcolumn_counter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCounter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\dataframe.py\u001B[0m in \u001B[0;36mcollect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1214\u001B[0m         \"\"\"\n\u001B[0;32m   1215\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1216\u001B[1;33m             \u001B[0msock_info\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcollectToPython\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1217\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mBatchedSerializer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mCPickleSerializer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1218\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1320\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1321\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1322\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1323\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1324\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    167\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    168\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 169\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    170\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    171\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    325\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 326\u001B[1;33m                 raise Py4JJavaError(\n\u001B[0m\u001B[0;32m    327\u001B[0m                     \u001B[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o399.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 25) (DESKTOP-9VU4VJO.home executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 27 more\r\n"
     ]
    }
   ],
   "source": [
    "# Define window function with partitionBy clause\n",
    "window = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "# Create time group table\n",
    "time_group_table = df.select(\"time_udf\", \"Amount\"). \\\n",
    "    groupBy(\"time_udf\"). \\\n",
    "    agg(\n",
    "    f.count(\"Amount\").alias(\"UserCount\"),\n",
    "    f.mean(\"Amount\").alias(\"Amount_Avg\"),\n",
    "    f.min(\"Amount\").alias(\"Amount_Min\"),\n",
    "    f.max(\"Amount\").alias(\"Amount_Max\")). \\\n",
    "    withColumn(\"total\", f.sum(\"UserCount\").over(window)). \\\n",
    "    withColumn(\"Percent\", f.round(f.col(\"UserCount\")*100 / f.col(\"total\"), 2)). \\\n",
    "    drop(\"total\"). \\\n",
    "    orderBy(f.desc(\"Percent\"))\n",
    "\n",
    "time_group_table.limit(10).toPandas()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:35:31.653136Z",
     "end_time": "2023-04-30T00:35:33.924863Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert PySpark dataframe to Pandas dataframe\n",
    "time_group_df = time_group_table.toPandas()\n",
    "\n",
    "# Create barplot\n",
    "ax = sns.barplot(x=\"time_udf\", y=\"Percent\", data=time_group_df)\n",
    "\n",
    "for p in ax.patches:\n",
    "    abs_value = p.get_height()\n",
    "    ax.annotate(f'{abs_value}',\n",
    "                (p.get_x() + p.get_width() / 2., abs_value),\n",
    "                ha='center', va='center',\n",
    "                xytext=(0, 9),\n",
    "                textcoords='offset points')\n",
    "\n",
    "plt.title(\"User Count and Transaction Amount by Time Group\")\n",
    "plt.xlabel(\"Time Group\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.show();"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:38:49.563788Z",
     "end_time": "2023-04-30T00:38:50.196215Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# All Transactions\n",
    "df_aux = df.select(\"Class\", \"Amount\").toPandas()\n",
    "\n",
    "# Define amount ranges\n",
    "amount_ranges = [\n",
    "    {\"range\": \"Transaction Value <= $100\", \"min_amount\": 0, \"max_amount\": 100},\n",
    "    {\"range\": \"Transaction Value between \\$101 and \\$2000\", \"min_amount\": 101, \"max_amount\": 2000},\n",
    "    {\"range\": \"Transaction Value between \\$2001 and \\$5000\", \"min_amount\": 2001, \"max_amount\": 5000},\n",
    "    {\"range\": \"Transaction Value > $5000\", \"min_amount\": 5001, \"max_amount\": df_aux[\"Amount\"].max()}\n",
    "]\n",
    "\n",
    "# Create four subplots for different amount ranges\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "for idx, amount_range in enumerate(amount_ranges):\n",
    "    row_idx = idx // 2\n",
    "    col_idx = idx % 2\n",
    "\n",
    "    # Filter transactions within amount range\n",
    "    df_range = df_aux[(df_aux[\"Amount\"] > amount_range[\"min_amount\"]) & (df_aux[\"Amount\"] <= amount_range[\"max_amount\"])]\n",
    "\n",
    "    # Plot histogram\n",
    "    sns.histplot(data=df_range, x=\"Amount\", ax=axes[row_idx, col_idx], hue=\"Class\", kde=True)\n",
    "    axes[row_idx, col_idx].set_title(amount_range[\"range\"])\n",
    "    axes[row_idx, col_idx].set_ylabel(\"Number of Transactions\")\n",
    "    axes[row_idx, col_idx].legend(labels=[\"Fraud\", \"Non-Fraud\"])\n",
    "\n",
    "fig.suptitle(\"All Transactions\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:40:40.027113Z",
     "end_time": "2023-04-30T00:40:44.478063Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Only fraud transactions\n",
    "only_fraud = df.filter(df.Class == 1).select(\"Amount\").toPandas()\n",
    "\n",
    "# Add a pallet\n",
    "aux_pal = [\"#ff7f7f\", \"#ff3c3c\"]\n",
    "\n",
    "# Create three subplots for different amount ranges\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "# Amount range <= 100\n",
    "sns.histplot(data=only_fraud[only_fraud[\"Amount\"] <= 100], x=\"Amount\", ax=axes[0], kde=True, color=aux_pal[0])\n",
    "axes[0].set_title(\"Transaction Amount <= $100\")\n",
    "axes[0].set_ylabel(\"Number of Transactions\")\n",
    "\n",
    "# Amount range > 100\n",
    "sns.histplot(data=only_fraud[(only_fraud[\"Amount\"] > 100)], x=\"Amount\", ax=axes[1], kde=True, color=aux_pal[1])\n",
    "axes[1].set_title(\"Transaction Amount > $100\")\n",
    "axes[1].set_ylabel(\"Number of Transactions\")\n",
    "\n",
    "fig.suptitle(\"Only Fraudulent Transactions\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:41:04.178356Z",
     "end_time": "2023-04-30T00:41:06.053233Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2.3 Remaining variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create several histograms to show the distributions of the features\n",
    "fig, axs = plt.subplots(8, 4, figsize=(25, 15))\n",
    "fig.suptitle(\"Distribution of Features\", fontsize=14)\n",
    "\n",
    "for col, ax in zip(df.columns, axs.flatten()):\n",
    "    ax.hist(df.select(col).toPandas()[col])\n",
    "    ax.grid(False)\n",
    "    ax.tick_params(axis='x', labelrotation=45, labelsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=15)\n",
    "    ax.set_title(col.upper(), fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9, hspace=0.4)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:42:16.924178Z",
     "end_time": "2023-04-30T00:43:18.843565Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Through the histograms, we can observe that the feature \"Time\" does not follow a normal distribution, with a lower occurrence in the range of 75,000-100,000 seconds. On the other hand, the features Vx show a normalized distribution centered around the value 0, with some negative values.\n",
    "\n",
    "## 1.3 Imbalanced Data\n",
    "\n",
    "To address the imbalanced data issue, we will implement the technique of Random Under Sampling. The aim is to balance the dataset by removing data, which helps to avoid model overfitting.\n",
    "\n",
    "The following steps will be taken:\n",
    "\n",
    "    - Determine the degree of imbalance in the class by using the \"value_counts()\" function on the class column to count the number of instances for each label.\n",
    "\n",
    "    - Bring the number of non-fraud transactions to the same amount as fraud transactions (assuming we want a 50/50 ratio),which will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.\n",
    "\n",
    "    - Shuffle the data to ensure that our models can maintain a certain accuracy every time we run the script.\n",
    "\n",
    "**Note**: It is important to note that the Random Under Sampling technique may result in a loss of information since we are reducing the number of instances in the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select fraud and non-fraud transactions and limit non-fraud transactions to the same number as fraud transactions\n",
    "fraud_df = df.filter(f.col('Class') == 1)\n",
    "non_fraud_df = df.filter(f.col('Class') == 0).limit(fraud_df.count())\n",
    "\n",
    "# Combine fraud and non-fraud transactions and shuffle the data\n",
    "balanced_df = fraud_df.union(non_fraud_df).orderBy(f.rand())\n",
    "\n",
    "# Show 10 rows of the shuffled, balanced dataframe\n",
    "balanced_df.limit(10).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:47:00.874878Z",
     "end_time": "2023-04-30T00:47:07.369452Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert the Spark dataframe to a pandas dataframe\n",
    "fraud_df = balanced_df.select(\"Class\").toPandas()\n",
    "\n",
    "# Create a countplot with logarithmic y-axis\n",
    "sns.countplot(x='Class', data=fraud_df, palette='Set1', order=[1, 0])\n",
    "\n",
    "# Calculate the percentage of each class\n",
    "fraud_counts = fraud_df['Class'].value_counts()\n",
    "fraud_perc = fraud_counts / fraud_counts.sum() * 100\n",
    "\n",
    "# Set the y-axis labels and formatter\n",
    "plt.ylabel('Number of Transactions')\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel('Transaction Class')\n",
    "plt.xticks([0, 1], ['Fraud', 'Non-Fraud'])\n",
    "plt.title('Balanced Distribution of Transactions')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T08:46:14.605477Z",
     "end_time": "2023-04-30T08:46:17.533215Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have our dataframe correctly balanced, we can go further with our analysis and data preprocessing.\n",
    "\n",
    "\n",
    "## Correlations\n",
    "\n",
    "Correlation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (Balanced) in order for us to see which features have a high positive or negative correlation in regard to fraud transactions.\n",
    "\n",
    "Summary and Explanation:\n",
    "\n",
    "    Negative Correlations: V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.\n",
    "    Positive Correlations: V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction.\n",
    "    BoxPlots: We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions.\n",
    "\n",
    "Note: We have to make sure we use the subsample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original dataframe.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\"time_udf\")\n",
    "balanced_df = balanced_df.drop(\"time_udf\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(df.toPandas().corr(), cmap='coolwarm_r')\n",
    "plt.title('Correlations with Unbalanced Data')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(balanced_df.toPandas().corr(), cmap='coolwarm_r')\n",
    "plt.title('Correlations with Balanced Data')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:53:53.414505Z",
     "end_time": "2023-04-30T00:54:01.986343Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(balanced_df.toPandas().corr() > 0.7,cbar=False, annot_kws={'size':20})\n",
    "plt.title('Correlations with Balanced Data > 0.7')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(balanced_df.toPandas().corr() < -0.7,cbar=False, annot_kws={'size':20})\n",
    "plt.title('Correlations with Balanced Data < -0.7')\n",
    "\n",
    "plt.show();"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:52:56.620180Z",
     "end_time": "2023-04-30T00:52:58.343689Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Conclusions from Correlation Matrix\n",
    "Based on our correlation matrix, we have concluded the following:\n",
    "\n",
    "- The variables that were a product of PCA are not correlated with each other, meaning they are independent.\n",
    "- The variable \"Time\" seems to be negatively correlated with all of the \"Vx\" variables, which means that as \"Time\" increases, the values of \"Vx\" decrease, and vice versa.\n",
    "- The variables \"Time\" and \"Class\" seem to have no correlation, indicating that the time at which a transaction occurs has no influence on whether it is fraudulent or not.\n",
    "- The variable \"Class\" seems to be negatively correlated with some of the \"Vx\" variables, indicating that certain values of \"Vx\" are more likely to be associated with fraudulent transactions, while not correlated at all with others.\n",
    "\n",
    "\n",
    "## Distributions: Univariate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select all columns except the ones to exclude\n",
    "cols_to_include = [col for col in balanced_df.columns if col not in ['Time']]\n",
    "df_to_plot = balanced_df.select(*cols_to_include)\n",
    "\n",
    "for col in df_to_plot.columns:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    # plot histogram\n",
    "    axs[0].hist(df_to_plot.select(col).rdd.flatMap(lambda x: x).collect(), bins=50)\n",
    "    axs[0].set_title(f\"Distribution of {col} values\", size=14)\n",
    "    axs[0].set_xlabel(col, size=12)\n",
    "    axs[0].set_ylabel(\"Count\", size=12)\n",
    "\n",
    "    # plot boxplot\n",
    "    sns.boxplot(data=df_to_plot.select(col).toPandas(), x=col, ax=axs[1])\n",
    "    axs[1].set_title(f\"{col} Boxplot\", size=14)\n",
    "    axs[1].set_xlabel(col, size=12)\n",
    "    axs[1].set_ylabel(\"Count\", size=12)\n",
    "\n",
    "plt.show();"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T00:57:58.174475Z",
     "end_time": "2023-04-30T01:01:17.929222Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distributions: Bivariate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bivariate Analysis\n",
    "\n",
    "data = df_to_plot.toPandas()\n",
    "fig, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
    "\n",
    "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=data,  ax=axes[0])\n",
    "axes[0].set_title('V14 vs Class Negative Correlation')\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V12\", data=data,  ax=axes[1])\n",
    "axes[1].set_title('V12 vs Class Negative Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V10\", data=data, ax=axes[2])\n",
    "axes[2].set_title('V10 vs Class Negative Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V16\", data=data, ax=axes[3])\n",
    "axes[3].set_title('V16 vs Class Negative Correlation')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T08:55:54.487960Z",
     "end_time": "2023-04-30T08:55:55.691778Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
    "\n",
    "# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\n",
    "sns.boxplot(x=\"Class\", y=\"V4\", data=data,  ax=axes[0])\n",
    "axes[0].set_title('V4 vs Class Positive Correlation')\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V11\", data=data,  ax=axes[1])\n",
    "axes[1].set_title('V11 vs Class Positive Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V2\", data=data, ax=axes[2])\n",
    "axes[2].set_title('V2 vs Class Positive Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V19\", data=data, ax=axes[3])\n",
    "axes[3].set_title('V19 vs Class Positive Correlation')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T08:50:18.783778Z",
     "end_time": "2023-04-30T08:50:19.264578Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hist_fraud = df_to_plot.filter(f.col('Class') == 1).drop('Class').toPandas()\n",
    "\n",
    "hist_fraud.hist(bins=30, figsize=(10, 10))\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1,\n",
    "                    right=0.9,\n",
    "                    top=0.9,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.8)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T09:00:33.163128Z",
     "end_time": "2023-04-30T09:00:42.356482Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
